{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiYSon9skUkT",
    "outputId": "60e7f99c-4e19-4831-d9a8-6ffee1536a97"
   },
   "outputs": [],
   "source": [
    "pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IGkOi-kUS9G"
   },
   "source": [
    "##Tokenization and Fine Tuning The T5-Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b74cc0b521e54a22a569b8e3884ee21a",
      "8e33188e1ae749bda157f766212602f1",
      "74b42615c26340f0a0c8c7585ac27576",
      "fe981d6a0d6f48a997d6b6ee71003a4c",
      "14728a07f85144fb9ffc0e34788b38bf",
      "62d5f95027b34359a2bf1caef2ece016",
      "ba9a9ca654e540e68757452d5763854d",
      "e7ad6f0756d54d59ad601b8474c9d293",
      "329b6fcc2f1341248179ae6fd0379b9a",
      "c6b2b88be2c447e3ad408d209d96990d",
      "c9a1e46131d64afba9897367225f937f",
      "edf6153c9a7c43abb8625878224a0a1f",
      "4b241122fd7140b9a659a55b391f38ec",
      "e055f3e74978410f9dd79e724f9f6588",
      "bb5f539e6b76458bb42f1d4b95518d64",
      "339fd600764041eca45026861f2b327b",
      "590bf5404ab24af3a3c4fe5d797545ec",
      "f39ae791d9724a0eb5a993722aa1355f",
      "587d20888d834256b1f1b972206121bd",
      "b997a527a1d24eccb693b0d544722477",
      "84924e2032f24684b1720dc837a36205",
      "78496469ba4442fca3e34eb85f1236c2",
      "fc47b93d87864ad283103f6712e5ac0a",
      "76c80e2549094776aa3cc29fb221367a",
      "d783de68fbe24fbabaa0b78eacf1639a",
      "f6a61d02680147df8ec1009e2aafa9a5",
      "9fc35a9d890f454fbba64fd186438e73",
      "8d87a0843a4c4735b85595969a57b3ec",
      "9d3609cb79a34299a0e76c1d6e44833d",
      "0eb571e63b5242dc8cda9db573bf48a3",
      "2f9176e73be7476c81616a9edba0a432",
      "f734358951394b658e9e39746ca1fc27",
      "8f2bb67de8974bdb892f50cc6966a56a",
      "00eac367e49e43039c871180eadb75dd",
      "224e981b87e8485e81225791cb664de4",
      "0d56726dda984922b01e3bff4fceeda4",
      "ee89c87e852d465d8d1d6c9dcff7132b",
      "ce261c75c5064aedb3cc7c3f8f06f005",
      "56c01fbb0ee240ff9bd6d02b88629aed",
      "7502312317844cc5b54760c47918e929",
      "724f53280d044dcba5931bf53ef03973",
      "2a43375fbddd4236af14094917aac52d",
      "137cbe4e16704c51843113d1a8b932f0",
      "7e043102a7294e2cbc6672695bdca440",
      "2be218e967f2479a9d06c904233f1272",
      "d05f2f3084f04ef1a066eaf65b7f1fa1",
      "4c2f23564d594727b9af49397fce1ddb",
      "f4aa8826efda4a13a821a564d085f087",
      "75ab035a8d024649be6baaaa04f28a32",
      "19ecd516f3a24d09a73eccc8853c9957",
      "a523a528407a4d7799992f887d92ae2b",
      "1b2b47feb9154de0ac8d8a6d85660fe9",
      "30ff0e926ce24bb6bb4571910c25d8a1",
      "066e09d68005457a99e1a2bfa467549d",
      "b190e74049344ded9c7b227743698c83",
      "c3f014021f624864a38f5ece008f725e",
      "37fce805b2e648078fa0b1ff6eb08cf2",
      "0d1890414ace4ff78bd20fe9367852d8",
      "60f28774ab6a43bb8a0f682d7c3f4c27",
      "ff1b35bb70d34409a68effb1aeb6fdc4",
      "04104008e6a24ef7bfd698262d118a0f",
      "28466ab73be443ebaf06b0eb307f4a03",
      "fae023885cd449d8b389f79e09d954b5",
      "348ad77fc50a469c937120eee98fc972",
      "5736e8d98105426c9e6128de0ccc27a5",
      "9636286bae544017b70cd27464b0c666"
     ]
    },
    "id": "nKmr94fPjz3v",
    "outputId": "fe029554-8cc7-48cb-9495-3f0ab95969f4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def clean_code(code):\n",
    "    \"\"\"Clean STM32 code by removing comments and normalizing whitespace.\"\"\"\n",
    "    code = re.sub(r\"//.*\", \"\", code)  # Remove single-line comments\n",
    "    code = re.sub(r\"/\\*.*?\\*/\", \"\", code, flags=re.DOTALL)  # Remove multi-line comments\n",
    "    code = re.sub(r'\\s+', ' ', code.strip())\n",
    "    return code\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_excel(\"/content/fonctions.xlsx\")\n",
    "    df = df[['Column2', 'Column3']].dropna()\n",
    "    df.columns = ['code', 'comment']\n",
    "    df['code'] = df['code'].apply(clean_code)\n",
    "    df['comment'] = df['comment'].apply(lambda x: x.strip())\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Step 2: Tokenization\n",
    "def tokenize_data(dataset, tokenizer):\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [\"generate comment: \" + code for code in examples['code']]\n",
    "        targets = examples['comment']\n",
    "        model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        model_inputs['code'] = examples['code']\n",
    "        model_inputs['comment'] = examples['comment']\n",
    "        return model_inputs\n",
    "\n",
    "    return dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Step 3: Model Training\n",
    "def train_model(train_dataset, val_dataset, model_name=\"t5-small\", output_dir=\"./t5_stm32_comment\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = tokenize_data(train_dataset, tokenizer)\n",
    "    val_dataset = tokenize_data(val_dataset, tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Step 4: Inference\n",
    "def generate_comment(code, model, tokenizer, device, max_length=128):\n",
    "    code = clean_code(code)\n",
    "    input_text = f\"generate comment: {code}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 5: Evaluation\n",
    "def evaluate_model(model, tokenizer, val_dataset, device):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    predictions, references = [], []\n",
    "\n",
    "    for example in val_dataset:\n",
    "        code = example['code']\n",
    "        ref = example['comment']\n",
    "        pred = generate_comment(code, model, tokenizer, device)\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    return {\"bleu\": bleu_score, \"rouge\": rouge_score}, predictions, references\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/content/fonctions.xlsx\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset, val_dataset = load_and_preprocess_data(file_path)\n",
    "    model, tokenizer = train_model(train_dataset, val_dataset)\n",
    "    model.to(device)\n",
    "\n",
    "    metrics, preds, refs = evaluate_model(model, tokenizer, val_dataset, device)\n",
    "    print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drUpyYKf_X8A"
   },
   "source": [
    "### 📉 Training and Validation Loss Analysis\n",
    "\n",
    "The model was trained over 20 epochs, and both the **training loss** and **validation loss** show a general decreasing trend, which indicates that the model is learning effectively. Here are key observations:\n",
    "\n",
    "- **Early Epochs (1–5):** Rapid decrease in both training and validation loss, suggesting that the model is quickly learning the basic structure of the task.\n",
    "- **Mid Epochs (6–12):** The losses continue to decrease, but at a slower rate, indicating a transition to more refined learning.\n",
    "- **Later Epochs (13–20):** The validation loss continues to decrease slightly and stabilizes around **0.742**, while training loss fluctuates. This suggests the model is converging.\n",
    "\n",
    "There is **no significant overfitting** observed, as the validation loss closely follows the training loss, and does not increase in later epochs. The best validation loss of **0.74225** is achieved at **epoch 20**, indicating this may be an optimal stopping point.\n",
    "\n",
    "Overall, the training process appears successful and stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vicpdaU4Wuz7"
   },
   "source": [
    "### ⚠️ Error Explanation and Resolution\n",
    "\n",
    "During model evaluation, we encountered an `ImportError` related to the ROUGE metric:\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "This error occurred because the rouge_score package, which is required for computing the ROUGE metric, was not installed in the environment. We resolved the Issur down below in The Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA_XE5WcUnVt"
   },
   "source": [
    "##Model Evaluation Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLZlWF55pBo1",
    "outputId": "c16d9acb-a49f-46b7-bb11-3cd68cec503e"
   },
   "outputs": [],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UEW6M8yRmXC"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate\n",
    "\n",
    "def evaluate_model(model, tokenizer, val_dataset, device):\n",
    "    \"\"\"Evaluate model performance using BLEU, ROUGE, and Cosine Similarity metrics.\"\"\"\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Load Sentence-BERT model for semantic similarity\n",
    "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    predictions, references, cosine_similarities = [], [], []\n",
    "\n",
    "    for example in val_dataset:\n",
    "        code = example['code']\n",
    "        reference = example['comment']\n",
    "\n",
    "        # Generate the predicted comment\n",
    "        try:\n",
    "            prediction = generate_comment(code, model, tokenizer, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating comment: {e}\")\n",
    "            prediction = \"\"\n",
    "\n",
    "        # Compute Cosine Similarity (semantic similarity)\n",
    "        ref_embedding = sentence_model.encode(reference)\n",
    "        pred_embedding = sentence_model.encode(prediction)\n",
    "        similarity = cosine_similarity([ref_embedding], [pred_embedding])[0][0]\n",
    "\n",
    "        # Append results\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "        cosine_similarities.append(similarity)\n",
    "\n",
    "    # Compute BLEU and ROUGE scores\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rouge\": rouge_score,\n",
    "        \"cosine_similarity\": cosine_similarities\n",
    "    }, predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772,
     "referenced_widgets": [
      "43dd31d34c4346cb98a90e74303fd879",
      "a908909f8bb041dbab8240a5b460608d",
      "430487e099724592b7262436e077f548",
      "dd4fccaea968459ebbc0cb8f5f789a2c",
      "8c1504db119d4e748014dba958ff09f6",
      "e230c97b7f9b42f19ef840d9155c8d49",
      "05c2ad89a329483b91d49258b1f5aa21",
      "1edc62674053428c92096265277ecea2",
      "cdcff1f276d346969ee43cba243d302f",
      "84ef61fb02624d16b11d3840d5629c29",
      "b769303318f049b596a00924ace1aaca",
      "f8e7525f50e64103880d71a2c98ff189",
      "13a8b30b64c040d9804a9dc4ef8f5037",
      "d992b6d2b31742159401af6305d35009",
      "78f4cf6ef97249dda722b0d4b68c6771",
      "6cd59411478940669875bf3e50aa65ac",
      "f0fef4d11acf4461a73237834c92629b",
      "a0aa64f4aa71490aa8e60881d1c94791",
      "80ee22f8dd244d96adba7a256b9e0197",
      "671222eb2c7b41ce8cffbeabb3c8f188",
      "45fb5af08ff1410ab7f873d1875db9e4",
      "3513fe6838d940a2a25679cbc6cbe473",
      "635cb8bf6f6f4f1ca9ea9d961b1c5693",
      "ca56b1ad271a49b8a2373409986c24b2",
      "97c2887c5d024f6d9939904c613665d1",
      "f3756c31753e4bc5a76816fb30ffe684",
      "368beae36517488bb45e8a0b69b4ebf2",
      "06a905d12f3b4bcf9fa31d582d39b078",
      "63d16598c2464c63bb0358a11a2335e4",
      "9bb93a3412f048dba4dc46ada2ed1c5a",
      "781f48eee978488caeb374c646673486",
      "2eb00921378d42aeb05b1b4f1883309f",
      "755708686ee64de9beacc72265762799",
      "4b160c4b90694a0f951dbbb6484c9610",
      "223907067a4c4a4f84423a5bdd8064b4",
      "7506ae2788fb4ddf84f5bc863a32536e",
      "1aa113315e4a4b678dd3f3b4df15be5e",
      "9884291156904a00ac6af7e810740b10",
      "9eb05f043e654b07be6f47eaf8955ad9",
      "5bba0f8dba5743409d02336711a9a0cd",
      "980d944a98b74d9d980a31d02e54f075",
      "7a3b09eb13f445119945a0623d98d5d3",
      "a576807ff2d246d5855a1bd413aa6e5e",
      "0a8dc7e636ef4881b15e8fe6b731fa99",
      "772e83f792e6459886d134583f65e738",
      "73e05e29ab8e4a71a031c23bdd20e856",
      "3f24cdfbdb6d4ab798d1ff444210efe2",
      "e1c781c6558e47bdbe7eff7d2d76aa34",
      "b2d5772766c3468d96500f75e0cc1bfd",
      "aa0d504b85574c569c34dc3c212c7e4f",
      "5756ef6240e84741a048f5e5a12269d4",
      "7f879726697e4c008187ce9da6d16aa3",
      "1ec28fab7c9b4468b2bc93a951430488",
      "b86fbcb1523d469ca4836d918b3699ab",
      "584af0291c4b4aef82547927cb398a8c",
      "e25b6cea65da413ba7607a3af855fe53",
      "736fcf70fbbe4fb8b56ec23fde85993d",
      "3950619951044e668073f4a275bbc22a",
      "afacb9061c4549afa2cde894f20dee05",
      "05694731b4724a88ab91910caa7ef745",
      "544a4b8507f4457eaec4ddb9bca8ed04",
      "9c57436de71e4d35a3daa617d3e15d10",
      "e945292843234fcaafd40b0691ec4f1e",
      "a84d5c2baed24e6b950c15ab9b94dfbd",
      "d05cf5ca2f754038bfbc4994d9c0ff8c",
      "2a3992622f2444d7bc67617676e99fc7",
      "a7e839beb2a34f85aa4fb6923b850970",
      "8b8f714365da44a682136ead2ee39793",
      "11b15c0cc61a4e18acf0499f3c541d2c",
      "0bb8ecdf97ff4dcf9280355de9cee262",
      "804597236335498b9198e0401b845718",
      "db94c32572e444fd815300c63c353fa0",
      "810ad6106f794139b1f6beb4ff5adf8e",
      "e0247a1770114792a7fc2d3b62e871e9",
      "b2b42f21448648b3a474ad8a6687f312",
      "698ddfe9753642468a8b8ffd9d255d10",
      "680bb4beb24f4fbdacf47fae22606388",
      "9760ec7754604eb982f0bee0419faa3d",
      "2078387efa2745ebb3ecd2a70f60fe9a",
      "c26946048ffb4dba8375ca88f9f3bbf5",
      "254a5df91b8e49838c865be5dbe0b3b4",
      "887416120887478f863bfc941f003de2",
      "915d03a6b9724b25a57e9e76cf0f8a10",
      "29b966d26bb649409fe392950eceffdd",
      "333b2af304744ae1bbbee924b7fe0587",
      "b15738885351409fb09da6a75aa785c2",
      "a820bf947b3543e5bb782ab92bf5d6f1",
      "0ded23e10e9c4f208a1dc88c76f217bf",
      "d3df40fcbca0429da0326868774ded2f",
      "2bd10e8ddffc4aeabebaf67f1ef27769",
      "195dca96164d40a99e544d38531ef6b5",
      "d38ca274fec246868dcdb81e052ef308",
      "2d6b81d7c49f4238a944654576feb1dd",
      "ec6037427a934fce8ed084be00c4c9bc",
      "f817e6755e864c22a04c3da059b50b0f",
      "2e445ae8b942421c861f8bfc466c553b",
      "7948079ffafd47b396e561644bb88c84",
      "f485d5aac31249f2a0a645653663f9f7",
      "bac2ef812d14403ea44456df84a91d9b",
      "a2a05edd404b4c3a9421849fb5466bd4",
      "eeea8061c2f64ecdb9a16f9055f67b5a",
      "784f63e8cfdf45cebc7d628d9f8d982f",
      "b7bc7149143a4b4ea74557c732bbf78d",
      "b7eb0832cf0f4b068350bdba38c0db06",
      "cc3753207e1d4f88bc44ee4c4a212456",
      "d2c6328f13d84b7a8cd9864233665142",
      "bf13d031826e4b77a05d9893bc1795d4",
      "e08fe76ddb0b4b14b5d67c33dbebfb37",
      "5157d723156d421b931047071078ddf4",
      "7b392d7c4c134b3e87baa4cfac1a0812",
      "93b9e5bf09e74ab39a6af9830bfb6c92",
      "d021df91e9974e7ebcbac276b11d7c4a",
      "6fba775b2f48493ab41a9c28b4ca6124",
      "0f0be05b94e7443aad6da8f60fb07e65",
      "53c0979a03c5415ca266b24a10222e92",
      "bbc2349ced914148ab5905a551481160",
      "8278436efa77469abd1abb4146a3047f",
      "ac2e1e43db984f8dacd3a6cd5c0c7b49",
      "41be3b1ad7104f8ab6a087600cd8b90a",
      "c7d4fa8a8be24694ab031f9f6657a7be",
      "2e0aa07188134b5ca0a2768a13d86e2c"
     ]
    },
    "id": "6s6E85ZcPy44",
    "outputId": "c6fe8ed0-3528-4fa1-aaa7-9c587ff70d71"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "metrics, predictions, references = evaluate_model(model, tokenizer, val_dataset, device)\n",
    "\n",
    "# Print all evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"BLEU Score:\", metrics['bleu'])\n",
    "print(\"ROUGE Score:\", metrics['rouge'])\n",
    "print(\"Cosine Similarity:\", metrics['cosine_similarity'])\n",
    "\n",
    "# Optional: print example outputs along with metrics\n",
    "for i in range(3):  # Print first 3 examples\n",
    "    print(\"\\nCode Sample:\", val_dataset[i]['code'])\n",
    "    print(\"True Comment:\", val_dataset[i]['comment'])\n",
    "    print(\"Predicted Comment:\", predictions[i])\n",
    "    print(\"Cosine Similarity:\", metrics['cosine_similarity'][i])  # Print semantic similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "pHcDxIyBX47Q",
    "outputId": "cbc8fb54-d356-4ddb-cc16-cce8c6c4859d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming metrics are available as follows:\n",
    "bleu_score = metrics['bleu']['bleu']\n",
    "rouge_scores = [\n",
    "    metrics['rouge']['rouge1'],\n",
    "    metrics['rouge']['rouge2'],\n",
    "    metrics['rouge']['rougeL']\n",
    "]\n",
    "\n",
    "# Calculate the average ROUGE score (mean of ROUGE-1, ROUGE-2, and ROUGE-L)\n",
    "average_rouge_score = np.mean(rouge_scores)\n",
    "\n",
    "# Average cosine similarity\n",
    "average_cosine_similarity = np.mean(metrics['cosine_similarity'])\n",
    "\n",
    "# Prepare the data for the bar chart\n",
    "labels = ['BLEU', 'ROUGE', 'Average Cosine Similarity']\n",
    "values = [bleu_score, average_rouge_score, average_cosine_similarity]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values, color=['blue', 'red', 'cyan'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Model Evaluation Metrics')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Display the bar chart\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuSdIqrYAQ9U"
   },
   "source": [
    "## 📊 Overall Evaluation Interpretation\n",
    "\n",
    "This section provides an interpretation of the model's performance based on three main evaluation metrics: **BLEU**, **ROUGE**, and **Average Cosine Similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔵 BLEU Score (Bilingual Evaluation Understudy)\n",
    "\n",
    "The **BLEU score** measures how closely the generated comments match the reference comments using n-gram overlap. In this evaluation, the BLEU score is moderately high, which indicates that:\n",
    "\n",
    "- The model captures the key vocabulary used in reference comments.\n",
    "- It may struggle slightly with syntactic fluency, as BLEU can penalize variations in phrasing.\n",
    "- A moderate BLEU score suggests that while the model generates relevant keywords, it may not always follow natural or diverse phrasings used by humans.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔴 ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "The **ROUGE scores**, particularly ROUGE-1, ROUGE-2, and ROUGE-L, provide insight into the recall-oriented quality of the model output:\n",
    "\n",
    "- **ROUGE-1** and **ROUGE-L** are reasonably strong, meaning the model successfully includes many of the important words and maintains logical word order from the reference.\n",
    "- **ROUGE-2**, however, is lower, indicating that the model finds it harder to reproduce accurate sequences of two or more words.\n",
    "- Overall, the ROUGE results show that the model is able to capture essential content but could improve fluency and contextual cohesion.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟢 Average Cosine Similarity (Semantic Similarity)\n",
    "\n",
    "The **Average Cosine Similarity** evaluates the semantic closeness between generated and reference comments by comparing their vector representations:\n",
    "\n",
    "- A **high cosine similarity** score reflects that, even when the exact wording differs, the **underlying meaning** of the model’s output is similar to the reference.\n",
    "- This is particularly important in cases where lexical diversity is desired but semantic fidelity is crucial.\n",
    "- The high score suggests that the model outputs are **semantically appropriate**, even if the wording or phrasing varies.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Final Assessment\n",
    "\n",
    "Combining these metrics, we conclude that the model performs **reasonably well**:\n",
    "- It captures relevant **keywords** (BLEU),\n",
    "- Preserves a **fair amount of structure and content** (ROUGE),\n",
    "- And generates outputs that are **semantically aligned** with the reference (Cosine Similarity).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1TTTnJYT5Ij"
   },
   "source": [
    "##Testing The Model : Generating Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8hDA3sctTiD",
    "outputId": "1705b5e4-5080-49e6-ec85-bcb7a7f4902a"
   },
   "outputs": [],
   "source": [
    "  # Example usage: Generate comment for new code\n",
    "sample_code = \"\"\"\n",
    "    int main(void)\n",
    "    {\n",
    "        HAL_Init();\n",
    "        SystemClock_Config();\n",
    "        MX_GPIO_Init();\n",
    "        MX_I2C1_Init();\n",
    "        while (1)\n",
    "        {\n",
    "            HAL_GPIO_TogglePin(GPIOA, GPIO_PIN_5);\n",
    "            HAL_Delay(500);\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "comment = generate_comment(sample_code, model, tokenizer,device)\n",
    "print(\"Sample Code Comment:\", comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLnAissytfWq",
    "outputId": "6aa07a4c-247d-4e6f-a61f-6029f3d69d54"
   },
   "outputs": [],
   "source": [
    "# Example usage: Generate comment for new code\n",
    "sample_code = \"\"\"\n",
    "void I2C_Write(uint8_t address, uint8_t data)\n",
    "{\n",
    "    HAL_I2C_Master_Transmit(&hi2c1, address, &data, 1, 1000);\n",
    "}\n",
    "\n",
    "    \"\"\"\n",
    "comment = generate_comment(sample_code, model, tokenizer,device)\n",
    "print(\"Sample Code Comment:\", comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ll24XJtg54qY",
    "outputId": "b2dd5158-8c8f-42c1-83ae-0f5f283c11d0"
   },
   "outputs": [],
   "source": [
    "# Example usage: Generate comment for new code\n",
    "sample_code = \"\"\"\n",
    "void UART_Init(void)\n",
    "{\n",
    "    huart2.Instance = USART2;\n",
    "    huart2.Init.BaudRate = 9600;\n",
    "    huart2.Init.WordLength = UART_WORDLENGTH_8B;\n",
    "    huart2.Init.StopBits = UART_STOPBITS_1;\n",
    "    huart2.Init.Parity = UART_PARITY_NONE;\n",
    "    huart2.Init.Mode = UART_MODE_TX_RX;\n",
    "    huart2.Init.HwFlowCtl = UART_HWCONTROL_NONE;\n",
    "    huart2.Init.OverSampling = UART_OVERSAMPLING_16;\n",
    "\n",
    "    if (HAL_UART_Init(&huart2) != HAL_OK)\n",
    "    {\n",
    "        Error_Handler();\n",
    "    }\n",
    "}\n",
    "\n",
    "    \"\"\"\n",
    "comment = generate_comment(sample_code, model, tokenizer,device)\n",
    "print(\"Sample Code Comment:\", comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FH8ASNBN8rw_",
    "outputId": "69a8b8a0-cc16-4f6a-d689-efcce8529112"
   },
   "outputs": [],
   "source": [
    "# Example usage: Generate comment for new code\n",
    "sample_code = \"\"\"\n",
    "int main(void)\n",
    "    {\n",
    "        HAL_Init();\n",
    "        SystemClock_Config();\n",
    "        MX_GPIO_Init();\n",
    "        MX_I2C1_Init();\n",
    "        while (1)\n",
    "        {\n",
    "            HAL_GPIO_TogglePin(GPIOA, GPIO_PIN_5);\n",
    "            HAL_Delay(500);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "comment = generate_comment(sample_code, model, tokenizer,device)\n",
    "print(\"Sample Code Comment:\", comment)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
