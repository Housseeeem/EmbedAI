{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"10d0a7e61ee64a48897168b41ee228e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d2a8094a8024ef5a900c9921467671f","IPY_MODEL_ba7ed9c5f5e8414583b249a58108a4a2","IPY_MODEL_3c36eb8707c642739bf519c11d235dd5"],"layout":"IPY_MODEL_b7c1cc7474f54d579d44e5499b9215b6"}},"9d2a8094a8024ef5a900c9921467671f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fa323a4d0554695b92f5b9c0f7c6931","placeholder":"​","style":"IPY_MODEL_7a059802ece6483bace57d6e5d63242c","value":"Filtering sequences: 100%"}},"ba7ed9c5f5e8414583b249a58108a4a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0dde1f6acdc46a2af02c584de742732","max":3847,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1a1c8305b944a33b57a464d1c4bc9e5","value":3847}},"3c36eb8707c642739bf519c11d235dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa2c4c2647bc4a74a82e3484d76ccd59","placeholder":"​","style":"IPY_MODEL_bee8d1ea0abf47be95e625e98a80cc40","value":" 3847/3847 [00:02&lt;00:00, 1309.22 examples/s]"}},"b7c1cc7474f54d579d44e5499b9215b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fa323a4d0554695b92f5b9c0f7c6931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a059802ece6483bace57d6e5d63242c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0dde1f6acdc46a2af02c584de742732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1a1c8305b944a33b57a464d1c4bc9e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa2c4c2647bc4a74a82e3484d76ccd59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bee8d1ea0abf47be95e625e98a80cc40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec03c8e9d5554c35927e78a3a64a0583":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1460dd4a7ab5466a921ac9deb74696e4","IPY_MODEL_e1d2d47293bf40cbaeb779c580cd67a4","IPY_MODEL_b523d64c465b44eca3f7670b31bc5b47"],"layout":"IPY_MODEL_f3253e6a9e85409783f2cd94a53dad49"}},"1460dd4a7ab5466a921ac9deb74696e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19c1c03628054e88991902cb65c82678","placeholder":"​","style":"IPY_MODEL_807a0ae62f3f4bd1a75b7ee68e527f6e","value":"Filtering sequences: 100%"}},"e1d2d47293bf40cbaeb779c580cd67a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24a15569409044b99c194d3967d09efb","max":3847,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f90f814545404c55a02dfcc18f13ad75","value":3847}},"b523d64c465b44eca3f7670b31bc5b47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f41c8fc2ea154d6a9c3e9607d4b1648b","placeholder":"​","style":"IPY_MODEL_22b96d9aa254422d9cf3ecd94a9c5d9b","value":" 3847/3847 [00:03&lt;00:00, 1083.14 examples/s]"}},"f3253e6a9e85409783f2cd94a53dad49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19c1c03628054e88991902cb65c82678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"807a0ae62f3f4bd1a75b7ee68e527f6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24a15569409044b99c194d3967d09efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90f814545404c55a02dfcc18f13ad75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f41c8fc2ea154d6a9c3e9607d4b1648b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22b96d9aa254422d9cf3ecd94a9c5d9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e6ba82cb82543b1a19c43682b99c416":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf625ec286764992b0b94b14745de223","IPY_MODEL_96c5c6de9eaa419e912fd247a32afb1f","IPY_MODEL_5f718f84d9644d51b05c58135c427472"],"layout":"IPY_MODEL_65b7539fec4543a3bb4763e96bd73788"}},"bf625ec286764992b0b94b14745de223":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2b67c78f4b749b187cf89950d8e772b","placeholder":"​","style":"IPY_MODEL_9d0456aa33004ef6b984a7ed2b905e58","value":"Filtering sequences: 100%"}},"96c5c6de9eaa419e912fd247a32afb1f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_048242300a354ce88921f9037ce43e5d","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdc72e0a342b4948a759f67339ba554b","value":481}},"5f718f84d9644d51b05c58135c427472":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7019aede0a5b4b30838f36c2ab3f00fd","placeholder":"​","style":"IPY_MODEL_052b1f2e00cb4b98b077d4c5b88c5061","value":" 481/481 [00:00&lt;00:00, 2160.13 examples/s]"}},"65b7539fec4543a3bb4763e96bd73788":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b67c78f4b749b187cf89950d8e772b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d0456aa33004ef6b984a7ed2b905e58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"048242300a354ce88921f9037ce43e5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdc72e0a342b4948a759f67339ba554b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7019aede0a5b4b30838f36c2ab3f00fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"052b1f2e00cb4b98b077d4c5b88c5061":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"487757a1dd614fd8815531eb671e3ec0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_075ee49c6f9a421284437ed28fb2af87","IPY_MODEL_cb1748a9592a4eb784ae46bff04f65e7","IPY_MODEL_e0f310d2a8624949bd2920320cf05932"],"layout":"IPY_MODEL_1cfd884ea49f4eb8890c57b45a245215"}},"075ee49c6f9a421284437ed28fb2af87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1259ff04bd44dc6a62e4c553b40c6b6","placeholder":"​","style":"IPY_MODEL_855d1498e20d418d977cb4cf03f61a9d","value":"Filtering sequences: 100%"}},"cb1748a9592a4eb784ae46bff04f65e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8b060ef0f2f44e99c8cc277f40c6740","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b91c919751240debedf79c4a5712ddd","value":481}},"e0f310d2a8624949bd2920320cf05932":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_322a0853c78644b390dea9e3b7b11375","placeholder":"​","style":"IPY_MODEL_6b041ec2772e464db31873ad582b6eb7","value":" 481/481 [00:00&lt;00:00, 2117.76 examples/s]"}},"1cfd884ea49f4eb8890c57b45a245215":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1259ff04bd44dc6a62e4c553b40c6b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855d1498e20d418d977cb4cf03f61a9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8b060ef0f2f44e99c8cc277f40c6740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b91c919751240debedf79c4a5712ddd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"322a0853c78644b390dea9e3b7b11375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b041ec2772e464db31873ad582b6eb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e058af0bfa17407d85eb0d37ae7bb511":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d12eaacccde445997cf6ea35b4b9f38","IPY_MODEL_b6cb4cf0bdd14fdaae8e543c87894f60","IPY_MODEL_3b71ab270f8d45f5975977fb70695624"],"layout":"IPY_MODEL_d960954d3d014d67be1c337f4c4d53bf"}},"6d12eaacccde445997cf6ea35b4b9f38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86b72047faa946db8332cbc294b2bb0d","placeholder":"​","style":"IPY_MODEL_f882545ac2234cc9b869025b91434530","value":"Map: 100%"}},"b6cb4cf0bdd14fdaae8e543c87894f60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62b9d7c0a9b044c1ae38889c5768e1d2","max":3055,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42f8c388a77448388e3917e97529199c","value":3055}},"3b71ab270f8d45f5975977fb70695624":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67d3791aabf14d59b29c7542704baebb","placeholder":"​","style":"IPY_MODEL_f51de7b8cabc4b53b98890fc589efa3f","value":" 3055/3055 [00:01&lt;00:00, 2134.69 examples/s]"}},"d960954d3d014d67be1c337f4c4d53bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b72047faa946db8332cbc294b2bb0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f882545ac2234cc9b869025b91434530":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62b9d7c0a9b044c1ae38889c5768e1d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42f8c388a77448388e3917e97529199c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67d3791aabf14d59b29c7542704baebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f51de7b8cabc4b53b98890fc589efa3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8010b53f260b4d4f9feb9eb71ac64d33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6a3fb68d3ad4063a3b5678c5eaf8ff9","IPY_MODEL_0255c2b51e44441ca30483504f8f777b","IPY_MODEL_4a3097f49b2f44eea48f3d195a374fab"],"layout":"IPY_MODEL_d8f4a8790997402d9558daf1a6e99108"}},"a6a3fb68d3ad4063a3b5678c5eaf8ff9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcb82598103344baac44a2ed57222126","placeholder":"​","style":"IPY_MODEL_f23e28cf266d49a59556f361dde70d97","value":"Map: 100%"}},"0255c2b51e44441ca30483504f8f777b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45220877402b4ab3b8da957a2d0594cf","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_151a276a84164f60a02aa0a5f6f67f51","value":350}},"4a3097f49b2f44eea48f3d195a374fab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ee2f9bb03144c859dbcd74c8549c1b3","placeholder":"​","style":"IPY_MODEL_9e6ec6d0a97a48d18084ba6c3732d568","value":" 350/350 [00:00&lt;00:00, 1359.62 examples/s]"}},"d8f4a8790997402d9558daf1a6e99108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb82598103344baac44a2ed57222126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f23e28cf266d49a59556f361dde70d97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45220877402b4ab3b8da957a2d0594cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"151a276a84164f60a02aa0a5f6f67f51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ee2f9bb03144c859dbcd74c8549c1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e6ec6d0a97a48d18084ba6c3732d568":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f644ce2cf5fb464585bf261b91d46605":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3578fd3b55044de08cad3850ed68c702","IPY_MODEL_5f792ce1f12841018e473909220152dd","IPY_MODEL_7419fc1b5475458cbe2e98e1897908fc"],"layout":"IPY_MODEL_ecf1bf5224c943dd9b8cbebb4779e697"}},"3578fd3b55044de08cad3850ed68c702":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f273325e3494c2cb56d0027fd0b9487","placeholder":"​","style":"IPY_MODEL_f2f966ccead045ee9020f52d01238552","value":"Map: 100%"}},"5f792ce1f12841018e473909220152dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df0b5731ac5342be90d1ce1e5e4ff104","max":374,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8fe08a0df453474e9e62166fd9f4d860","value":374}},"7419fc1b5475458cbe2e98e1897908fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22172ebcc7e046e294fd2633ff4ad827","placeholder":"​","style":"IPY_MODEL_00adf9dccc3642d48ac32e2fbd652441","value":" 374/374 [00:00&lt;00:00, 1765.43 examples/s]"}},"ecf1bf5224c943dd9b8cbebb4779e697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f273325e3494c2cb56d0027fd0b9487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2f966ccead045ee9020f52d01238552":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df0b5731ac5342be90d1ce1e5e4ff104":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fe08a0df453474e9e62166fd9f4d860":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22172ebcc7e046e294fd2633ff4ad827":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00adf9dccc3642d48ac32e2fbd652441":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6667efb53334034bed0746af98d09b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48c4752dcc3f484695abc96a07548d54","IPY_MODEL_536fa1ee22fe4e70a39cce0a0dda2ab4","IPY_MODEL_87f6e17763dd4a54808b500897b55121"],"layout":"IPY_MODEL_69cdddf5640e40659fedfda0868085e1"}},"48c4752dcc3f484695abc96a07548d54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_933747f857114790b0f6ed148c2d929f","placeholder":"​","style":"IPY_MODEL_1b089ac25e3c432693a156eb9e73c3ec","value":"Map: 100%"}},"536fa1ee22fe4e70a39cce0a0dda2ab4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6ea311d5a84d4699a5634832c2caef","max":3055,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca41222597124af4b74aec7a0b517606","value":3055}},"87f6e17763dd4a54808b500897b55121":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce567ab079d24917b6f7f10517804915","placeholder":"​","style":"IPY_MODEL_1fcd70c64b954c4f860f4f1516f2b94c","value":" 3055/3055 [00:00&lt;00:00, 3772.30 examples/s]"}},"69cdddf5640e40659fedfda0868085e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"933747f857114790b0f6ed148c2d929f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b089ac25e3c432693a156eb9e73c3ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e6ea311d5a84d4699a5634832c2caef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca41222597124af4b74aec7a0b517606":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce567ab079d24917b6f7f10517804915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fcd70c64b954c4f860f4f1516f2b94c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82f6b5a079674d53a818fe12f0fc1417":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e5bd3b3e96d429bbaae37f1505b5b8b","IPY_MODEL_de61025093b9432da804ff9dfcc033db","IPY_MODEL_406462f234cd42ab87ca5a6270b85ee8"],"layout":"IPY_MODEL_db01ba0c2cd244628764e94e0d91c4d7"}},"2e5bd3b3e96d429bbaae37f1505b5b8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d5e9e85f0eb4cc4bff183b9174f578a","placeholder":"​","style":"IPY_MODEL_be9baa0a8279483f84d4d109bed26442","value":"Map: 100%"}},"de61025093b9432da804ff9dfcc033db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b7a4ed97add4165892df3bdd8bc2dc6","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_546ae15f7b48490b95445e81fd1824aa","value":350}},"406462f234cd42ab87ca5a6270b85ee8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffa4d5b1cb464daa89b64cf996175c9c","placeholder":"​","style":"IPY_MODEL_91a3fdefa3a54b5d9449c519a2254053","value":" 350/350 [00:00&lt;00:00, 717.08 examples/s]"}},"db01ba0c2cd244628764e94e0d91c4d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d5e9e85f0eb4cc4bff183b9174f578a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be9baa0a8279483f84d4d109bed26442":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b7a4ed97add4165892df3bdd8bc2dc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"546ae15f7b48490b95445e81fd1824aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffa4d5b1cb464daa89b64cf996175c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91a3fdefa3a54b5d9449c519a2254053":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fff5d79be9f54903b98252d5f987cd87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d11e4f9846144d6b0bf16ea99945b13","IPY_MODEL_5771daa09c3a4b5a869e663349a1336c","IPY_MODEL_11604d6e8f8e44c8ba9a11dad31375ac"],"layout":"IPY_MODEL_629fc84932c84dd99792bc741f4fff30"}},"2d11e4f9846144d6b0bf16ea99945b13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef5e772224e64d88afd46757118b8ef8","placeholder":"​","style":"IPY_MODEL_8d735abd86934e6bbb56dbc1bbf809c8","value":"Map: 100%"}},"5771daa09c3a4b5a869e663349a1336c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f1b9f5a29b24f15ad761aab9e5c2c5d","max":374,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ca609d8e8154186bd4b2158ecb3e402","value":374}},"11604d6e8f8e44c8ba9a11dad31375ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c08eababe8254ce581c59d3d61638d60","placeholder":"​","style":"IPY_MODEL_8de4363f820f499aa49735450fea9e3c","value":" 374/374 [00:00&lt;00:00, 1552.37 examples/s]"}},"629fc84932c84dd99792bc741f4fff30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef5e772224e64d88afd46757118b8ef8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d735abd86934e6bbb56dbc1bbf809c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f1b9f5a29b24f15ad761aab9e5c2c5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ca609d8e8154186bd4b2158ecb3e402":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c08eababe8254ce581c59d3d61638d60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de4363f820f499aa49735450fea9e3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11766136,"sourceType":"datasetVersion","datasetId":7386633}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.44.2 datasets==2.20.0 peft==0.12.0 torch==2.6.0 numpy==1.26.4 scikit-learn==1.5.2 nltk -q\n!pip install torchvision==0.21.0 torchaudio==2.6.0 -q  # Match torch version\n!pip install fsspec==2025.3.2 -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRAfjIv0DHQ7","outputId":"7d552558-7efb-4c6b-c4e3-1a38483f0a30","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:03:45.300984Z","iopub.execute_input":"2025-05-12T09:03:45.301492Z","iopub.status.idle":"2025-05-12T09:06:28.822880Z","shell.execute_reply.started":"2025-05-12T09:03:45.301466Z","shell.execute_reply":"2025-05-12T09:06:28.821879Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.5.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom transformers import RobertaTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model\nimport torch\nimport os","metadata":{"id":"a6Px0BdRDKTQ","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:28.824614Z","iopub.execute_input":"2025-05-12T09:06:28.824867Z","iopub.status.idle":"2025-05-12T09:06:48.512538Z","shell.execute_reply.started":"2025-05-12T09:06:28.824845Z","shell.execute_reply":"2025-05-12T09:06:48.511962Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 09:06:36.391382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747040796.564210      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747040796.614021      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Path to the uploaded dataset\ndataset_path = '/kaggle/input/stm-32-clean-berasmi/stm32_clean.csv'\ndf = pd.read_csv(dataset_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"Q7G-MTexDMX4","outputId":"1102ecc5-a803-4f57-c720-75b0dd84cb69","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:48.513246Z","iopub.execute_input":"2025-05-12T09:06:48.513958Z","iopub.status.idle":"2025-05-12T09:06:48.581106Z","shell.execute_reply.started":"2025-05-12T09:06:48.513930Z","shell.execute_reply":"2025-05-12T09:06:48.580497Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"id":"Jsp7GMJMDMa7","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:48.583002Z","iopub.execute_input":"2025-05-12T09:06:48.583266Z","iopub.status.idle":"2025-05-12T09:06:48.905062Z","shell.execute_reply.started":"2025-05-12T09:06:48.583247Z","shell.execute_reply":"2025-05-12T09:06:48.904339Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"formulating prompt (input) / code (output)","metadata":{}},{"cell_type":"code","source":"# Step 2\ndef format_data(row):\n    input_text = f\"Generate STM32 code for the following description: {row['comment']}\"\n    target_text = row['code']\n    return pd.Series([input_text, target_text], index=['input_text', 'target_text'])\n\ndf_formatted = df.apply(format_data, axis=1)\ndf_formatted = df_formatted.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"id":"94N7K9FLDMdx","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:48.905886Z","iopub.execute_input":"2025-05-12T09:06:48.906083Z","iopub.status.idle":"2025-05-12T09:06:49.549385Z","shell.execute_reply.started":"2025-05-12T09:06:48.906067Z","shell.execute_reply":"2025-05-12T09:06:49.547845Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df_formatted = df_formatted.dropna(subset=['input_text', 'target_text'])  # Remove NaN/None\ndf_formatted = df_formatted[df_formatted['input_text'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\ndf_formatted = df_formatted[df_formatted['target_text'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]","metadata":{"id":"n_wpVi1wDMgp","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:49.550556Z","iopub.execute_input":"2025-05-12T09:06:49.550895Z","iopub.status.idle":"2025-05-12T09:06:49.564696Z","shell.execute_reply.started":"2025-05-12T09:06:49.550872Z","shell.execute_reply":"2025-05-12T09:06:49.563800Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"train/test","metadata":{}},{"cell_type":"code","source":"# Step 3\ntrain_df, temp_df = train_test_split(df_formatted, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)","metadata":{"id":"HVoz6qJPDMjK","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:49.565601Z","iopub.execute_input":"2025-05-12T09:06:49.565857Z","iopub.status.idle":"2025-05-12T09:06:49.583882Z","shell.execute_reply.started":"2025-05-12T09:06:49.565836Z","shell.execute_reply":"2025-05-12T09:06:49.583163Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Step 4: Convert to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\nval_dataset = Dataset.from_pandas(val_df[['input_text', 'target_text']])\ntest_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])","metadata":{"id":"hgpppjlwDMmA","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:49.584702Z","iopub.execute_input":"2025-05-12T09:06:49.584969Z","iopub.status.idle":"2025-05-12T09:06:49.651984Z","shell.execute_reply.started":"2025-05-12T09:06:49.584936Z","shell.execute_reply":"2025-05-12T09:06:49.651450Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"load model","metadata":{}},{"cell_type":"code","source":"# Step 5: Load Model and Tokenizer\nmodel_name = \"Salesforce/codet5-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9js89AZZDMox","outputId":"4e9ace16-bfa4-45d9-8360-744c57bcd194","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:49.652755Z","iopub.execute_input":"2025-05-12T09:06:49.652966Z","iopub.status.idle":"2025-05-12T09:06:55.183133Z","shell.execute_reply.started":"2025-05-12T09:06:49.652949Z","shell.execute_reply":"2025-05-12T09:06:55.182577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3c4be62a7747b688a29284a9ae92c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"183a19151a6d425aa5c7815920b89a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6c535e07f4a48359858e097c5f9c8d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d01b3e44be409780ca37ba46c9bed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92dcef4a37243528108d601ed8094ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc283da8f9134101863ab4395655103b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"163c65de57a040eb9950a3aaad599b82"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"sequence info","metadata":{}},{"cell_type":"code","source":"combined_df = pd.concat(\n    [train_df[['input_text', 'target_text']],\n     val_df[['input_text', 'target_text']],\n     test_df[['input_text', 'target_text']]],\n    ignore_index=True\n)\n\n# Convert to Hugging Face Dataset\ncombined_dataset = Dataset.from_pandas(combined_df)\ndef analyze_sequence_lengths(dataset, tokenizer):\n    input_lengths = []\n    target_lengths = []\n    input_examples = []\n    target_examples = []\n\n    # Collect token lengths and examples\n    for example in dataset:\n        input_tokens = tokenizer(example['input_text'], truncation=False, padding=False)['input_ids']\n        target_tokens = tokenizer(example['target_text'], truncation=False, padding=False)['input_ids']\n        input_len = len(input_tokens)\n        target_len = len(target_tokens)\n        input_lengths.append(input_len)\n        target_lengths.append(target_len)\n        input_examples.append((input_len, example['input_text']))\n        target_examples.append((target_len, example['target_text']))\n\n    # Compute averages\n    avg_input_length = sum(input_lengths) / len(input_lengths)\n    avg_target_length = sum(target_lengths) / len(target_lengths)\n\n    # Get top 5 longest sequences\n    top_5_inputs = sorted(input_examples, key=lambda x: x[0], reverse=True)[:5]\n    top_5_targets = sorted(target_examples, key=lambda x: x[0], reverse=True)[:5]\n\n    # Print results\n    print(f\"\\nAverage Sequence Lengths (in tokens):\")\n    print(f\"Input (prompts): {avg_input_length:.2f} tokens\")\n    print(f\"Target (code): {avg_target_length:.2f} tokens\")\n\n    print(f\"\\nTop 5 Longest Input Sequences:\")\n    for i, (length, text) in enumerate(top_5_inputs, 1):\n        print(f\"{i}. Length: {length} tokens, Text: {text}\")\n\n    print(f\"\\nTop 5 Longest Target Sequences:\")\n    for i, (length, text) in enumerate(top_5_targets, 1):\n        print(f\"{i}. Length: {length} tokens, Text: {text}\")\n\n# Run analysis\nanalyze_sequence_lengths(combined_dataset, tokenizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4tP-NM8WSUh","outputId":"7f7dadb5-bfe6-4998-a3b0-5e2d48835c8a","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T00:21:01.006826Z","iopub.execute_input":"2025-05-12T00:21:01.007116Z","iopub.status.idle":"2025-05-12T00:21:02.999319Z","shell.execute_reply.started":"2025-05-12T00:21:01.007099Z","shell.execute_reply":"2025-05-12T00:21:02.998685Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Sequence Lengths (in tokens):\nInput (prompts): 20.57 tokens\nTarget (code): 45.14 tokens\n\nTop 5 Longest Input Sequences:\n1. Length: 103 tokens, Text: Generate STM32 code for the following description: Check the availability of adafruit 1.8\" TFT shield on top of STM32NUCLEO board. This is done by reading the state of IO PF.03 pin (mapped to JoyStick available on adafruit 1.8\" TFT shield). If the state of PF.03 is high then the adafruit 1.8\" TFT shield is available.\n2. Length: 103 tokens, Text: Generate STM32 code for the following description: Check the availability of adafruit 1.8\" TFT shield on top of STM32NUCLEO board. This is done by reading the state of IO PF.03 pin (mapped to JoyStick available on adafruit 1.8\" TFT shield). If the state of PF.03 is high then the adafruit 1.8\" TFT shield is available.\n3. Length: 83 tokens, Text: Generate STM32 code for the following description: STOP Mode Entry: RTC Clocked by LSI, Regulator in LP mode, HSI, HSE OFF and LSI OFF if not used as RTC Clock source, No IWDG, FLASH in deep power down mode, Automatic Wake-up using RTC clocked by LSI (after ~20s)\n4. Length: 77 tokens, Text: Generate STM32 code for the following description: TIM1 CH1 (PE9,FMU_CH4), TIM1 CH2 (PE11,FMU_CH3), TIM1 CH3 (PE13,FMU_CH2) and TIM1 CH4 (PE14,FMU_CH1)\n5. Length: 71 tokens, Text: Generate STM32 code for the following description: Period elapsed callback in non blocking mode - This function is called when TIM17 interrupt took place, inside HAL_TIM_IRQHandler(). It makes a direct call to HAL_IncTick() to increment a global variable \"uwTick\" used as application time base.\n\nTop 5 Longest Target Sequences:\n1. Length: 1819 tokens, Text: void initClock(void) {    RCC_ClkInitTypeDef RCC_ClkInitStruct;    RCC_OscInitTypeDef RCC_OscInitStruct;#if defined STM32F1    __HAL_RCC_PWR_CLK_ENABLE();    uint8_t fLatency;    RCC_OscInitStruct.OscillatorType  = RCC_OSCILLATORTYPE_HSE;    RCC_OscInitStruct.HSEState = RCC_HSE_ON;    RCC_OscInitStruct.LSEState = RCC_LSE_OFF;    RCC_OscInitStruct.HSIState = RCC_HSI_OFF;    RCC_OscInitStruct.HSICalibrationValue = 0;    RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;    RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;# if (defined STM32F100xB) || (defined STM32F100xE)    RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;    RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL3;    fLatency = FLASH_LATENCY_0;# elif (defined STM32F101x6) || (defined STM32F101xB) || (defined STM32F101xE) || (defined STM32F101xG)    RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV2;    RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL9;    fLatency = FLASH_LATENCY_1;# elif (defined STM32F102x6) || (defined STM32F102xB)    RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;    RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL6;    fLatency = FLASH_LATENCY_1;# elif (defined STM32F103x6) || (defined STM32F103xB) || (defined STM32F103xE) || (defined STM32F103xG)    RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;    RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL9;    fLatency = FLASH_LATENCY_2;# elif (defined STM32F105xC) || (defined STM32F107xC)    RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;    RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL9;    fLatency = FLASH_LATENCY_2;# endif    HAL_RCC_OscConfig(&RCC_OscInitStruct);    RCC_ClkInitStruct.ClockType = (RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK | RCC_CLOCKTYPE_PCLK1 | RCC_CLOCKTYPE_PCLK2);    RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;    RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;    RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV1;    RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV2;    HAL_RCC_ClockConfig(&RCC_ClkInitStruct, fLatency);#elif defined STM32F2    RCC_OscInitStruct.OscillatorType = RCC_OSCILLATORTYPE_HSE;    RCC_OscInitStruct.HSEState = RCC_HSE_ON;    RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;    RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;    RCC_OscInitStruct.PLL.PLLM = 25;    RCC_OscInitStruct.PLL.PLLN = 240;    RCC_OscInitStruct.PLL.PLLP = RCC_PLLP_DIV2;    RCC_OscInitStruct.PLL.PLLQ = 5;    HAL_RCC_OscConfig(&RCC_OscInitStruct);    RCC_ClkInitStruct.ClockType = (RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK | RCC_CLOCKTYPE_PCLK1 | RCC_CLOCKTYPE_PCLK2);    RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;    RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;    RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV4;    RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV2;    HAL_RCC_ClockConfig(&RCC_ClkInitStruct, FLASH_LATENCY_3);#elif defined STM32F4    __HAL_RCC_PWR_CLK_ENABLE();    __HAL_PWR_VOLTAGESCALING_CONFIG(PWR_REGULATOR_VOLTAGE_SCALE1);    RCC_OscInitStruct.OscillatorType = RCC_OSCILLATORTYPE_HSE;    RCC_OscInitStruct.HSEState = RCC_HSE_ON;    RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;    RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;    RCC_OscInitStruct.PLL.PLLM = 8;    RCC_OscInitStruct.PLL.PLLN = 336;    RCC_OscInitStruct.PLL.PLLP = RCC_PLLP_DIV2;    RCC_OscInitStruct.PLL.PLLQ = 7;    HAL_RCC_OscConfig(&RCC_OscInitStruct);    RCC_ClkInitStruct.ClockType = (RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK | RCC_CLOCKTYPE_PCLK1 | RCC_CLOCKTYPE_PCLK2);    RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;    RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;    RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV2;    RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV4;    HAL_RCC_ClockConfig(&RCC_ClkInitStruct, FLASH_LATENCY_5);    if (HAL_GetREVID() == 0x1001)    {        __HAL_FLASH_PREFETCH_BUFFER_ENABLE();    }#endif}\n2. Length: 1097 tokens, Text: static int btstack_uart_block_stm32_hal_init(const btstack_uart_config_t *config) { const pbdrv_bluetooth_btstack_uart_block_stm32_platform_data_t *pdata = &pbdrv_bluetooth_btstack_uart_block_stm32_platform_data; uart_config = config; btstack_tx_hdma.Instance = pdata->tx_dma; btstack_tx_hdma.Init.Channel = pdata->tx_dma_ch; btstack_tx_hdma.Init.Direction = DMA_MEMORY_TO_PERIPH; btstack_tx_hdma.Init.PeriphInc = DMA_PINC_DISABLE; btstack_tx_hdma.Init.MemInc = DMA_MINC_ENABLE; btstack_tx_hdma.Init.PeriphDataAlignment = DMA_PDATAALIGN_BYTE; btstack_tx_hdma.Init.MemDataAlignment = DMA_MDATAALIGN_BYTE; btstack_tx_hdma.Init.Mode = DMA_NORMAL; btstack_tx_hdma.Init.Priority = DMA_PRIORITY_VERY_HIGH; btstack_tx_hdma.Init.FIFOMode = DMA_FIFOMODE_DISABLE; btstack_tx_hdma.Init.FIFOThreshold = DMA_FIFO_THRESHOLD_1QUARTERFULL; btstack_tx_hdma.Init.MemBurst = DMA_MBURST_SINGLE; btstack_tx_hdma.Init.PeriphBurst = DMA_PBURST_SINGLE; HAL_DMA_Init(&btstack_tx_hdma); btstack_rx_hdma.Instance = pdata->rx_dma; btstack_rx_hdma.Init.Channel = pdata->rx_dma_ch; btstack_rx_hdma.Init.Direction = DMA_PERIPH_TO_MEMORY; btstack_rx_hdma.Init.PeriphInc = DMA_PINC_DISABLE; btstack_rx_hdma.Init.MemInc = DMA_MINC_ENABLE; btstack_rx_hdma.Init.PeriphDataAlignment = DMA_PDATAALIGN_BYTE; btstack_rx_hdma.Init.MemDataAlignment = DMA_MDATAALIGN_BYTE; btstack_rx_hdma.Init.Mode = DMA_NORMAL; btstack_rx_hdma.Init.Priority = DMA_PRIORITY_VERY_HIGH; btstack_rx_hdma.Init.FIFOMode = DMA_FIFOMODE_DISABLE; btstack_rx_hdma.Init.FIFOThreshold = DMA_FIFO_THRESHOLD_1QUARTERFULL; btstack_rx_hdma.Init.MemBurst = DMA_MBURST_SINGLE; btstack_rx_hdma.Init.PeriphBurst = DMA_PBURST_SINGLE; HAL_DMA_Init(&btstack_rx_hdma); btstack_huart.Instance = pdata->uart; btstack_huart.Init.BaudRate = config->baudrate; btstack_huart.Init.WordLength = UART_WORDLENGTH_8B; btstack_huart.Init.StopBits = UART_STOPBITS_1; btstack_huart.Init.Parity = UART_PARITY_NONE; btstack_huart.Init.Mode = UART_MODE_TX_RX; btstack_huart.Init.HwFlowCtl = config->flowcontrol ? UART_HWCONTROL_RTS_CTS : UART_HWCONTROL_NONE; btstack_huart.Init.OverSampling = UART_OVERSAMPLING_16; HAL_UART_Init(&btstack_huart); __HAL_LINKDMA(&btstack_huart, hdmatx, btstack_tx_hdma); __HAL_LINKDMA(&btstack_huart, hdmarx, btstack_rx_hdma); HAL_NVIC_SetPriority(pdata->tx_dma_irq, 1, 2); HAL_NVIC_EnableIRQ(pdata->tx_dma_irq); HAL_NVIC_SetPriority(pdata->rx_dma_irq, 1, 1); HAL_NVIC_EnableIRQ(pdata->rx_dma_irq); HAL_NVIC_SetPriority(pdata->uart_irq, 1, 0); HAL_NVIC_EnableIRQ(pdata->uart_irq); return 0; }\n3. Length: 929 tokens, Text: void SystemClock_Config_HSE(uint32_t clock_freq) { RCC_OscInitTypeDef osc_init; RCC_ClkInitTypeDef clk_init; uint32_t fLatency; osc_init.OscillatorType = RCC_OSCILLATORTYPE_HSE; osc_init.HSEState = RCC_HSE_ON; osc_init.PLL.PLLSource = RCC_PLLSOURCE_HSE; clk_init.ClockType = RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK | RCC_CLOCKTYPE_PCLK1 | RCC_CLOCKTYPE_PCLK2; clk_init.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK; switch (clock_freq) { case SYS_CLOCK_FREQ_50_MHZ: osc_init.PLL.PLLM = 8; osc_init.PLL.PLLN = 100; osc_init.PLL.PLLP = RCC_PLLP_DIV2; osc_init.PLL.PLLState = RCC_PLL_ON; clk_init.AHBCLKDivider = RCC_SYSCLK_DIV1; clk_init.APB1CLKDivider = RCC_HCLK_DIV1; clk_init.APB2CLKDivider = RCC_HCLK_DIV2; fLatency = FLASH_LATENCY_1; break; case SYS_CLOCK_FREQ_84_MHZ: osc_init.PLL.PLLM = 4; osc_init.PLL.PLLN = 84; osc_init.PLL.PLLP = RCC_PLLP_DIV2; osc_init.PLL.PLLState = RCC_PLL_ON; clk_init.AHBCLKDivider = RCC_SYSCLK_DIV1; clk_init.APB1CLKDivider = RCC_HCLK_DIV2; clk_init.APB2CLKDivider = RCC_HCLK_DIV1; fLatency = FLASH_LATENCY_2; break; case SYS_CLOCK_FREQ_120MHZ: osc_init.PLL.PLLM = 4; osc_init.PLL.PLLN = 120; osc_init.PLL.PLLP = RCC_PLLP_DIV2; osc_init.PLL.PLLState = RCC_PLL_ON; clk_init.AHBCLKDivider = RCC_SYSCLK_DIV1; clk_init.APB1CLKDivider = RCC_HCLK_DIV4; clk_init.APB2CLKDivider = RCC_HCLK_DIV2; fLatency = FLASH_LATENCY_3; break; case SYS_CLOCK_FREQ_180MHZ: osc_init.PLL.PLLM = 4; osc_init.PLL.PLLN = 180; osc_init.PLL.PLLP = RCC_PLLP_DIV2; osc_init.PLL.PLLState = RCC_PLL_ON; clk_init.AHBCLKDivider = RCC_SYSCLK_DIV1; clk_init.APB1CLKDivider = RCC_HCLK_DIV4; clk_init.APB2CLKDivider = RCC_HCLK_DIV2; fLatency = FLASH_LATENCY_5; break; } HAL_RCC_OscConfig(&osc_init); HAL_RCC_ClockConfig(&clk_init, fLatency); HAL_SYSTICK_Config(HAL_RCC_GetHCLKFreq() / 1000); HAL_SYSTICK_CLKSourceConfig(SYSTICK_CLKSOURCE_HCLK); }\n4. Length: 827 tokens, Text: static int app_collect_task_entry() { Init_E53_SC2(); while (1) { E53_SC2_Read_Data(); printf(\"\\r\\n******************************Temperature      is  %d\\r\\n\", (int)E53_SC2_Data.Temperature); printf(\"\\r\\n******************************Accel[0]         is  %d\\r\\n\", (int)E53_SC2_Data.Accel[0]); printf(\"\\r\\n******************************Accel[1]         is  %d\\r\\n\", (int)E53_SC2_Data.Accel[1]); printf(\"\\r\\n******************************Accel[2]         is  %d\\r\\n\", (int)E53_SC2_Data.Accel[2]); if( X == 0 && Y == 0 && Z == 0) { X = (int)E53_SC2_Data.Accel[0]; Y = (int)E53_SC2_Data.Accel[1]; Z = (int)E53_SC2_Data.Accel[2]; } else { if(X+100<E53_SC2_Data.Accel[0]||X-100>E53_SC2_Data.Accel[0]||Y+100<E53_SC2_Data.Accel[1]||Y-100>E53_SC2_Data.Accel[1]||Z+100<E53_SC2_Data.Accel[2]||Z-100>E53_SC2_Data.Accel[2]) { HAL_GPIO_WritePin(GPIOB,GPIO_PIN_8,GPIO_PIN_SET); HAL_GPIO_WritePin(GPIOB,GPIO_PIN_9,GPIO_PIN_RESET); Manhole_Cover.Status[0] = ' '; Manhole_Cover.Status[1] = 'T'; Manhole_Cover.Status[2] = 'i'; Manhole_Cover.Status[3] = 'l'; Manhole_Cover.Status[4] = 't'; } else { HAL_GPIO_WritePin(GPIOB,GPIO_PIN_8,GPIO_PIN_RESET); HAL_GPIO_WritePin(GPIOB,GPIO_PIN_9,GPIO_PIN_SET); Manhole_Cover.Status[0] = 'L'; Manhole_Cover.Status[1] = 'e'; Manhole_Cover.Status[2] = 'v'; Manhole_Cover.Status[3] = 'e'; Manhole_Cover.Status[4] = 'l'; } } LCD_ShowString(10, 135, 200, 16, 16, \"Temperature:\"); LCD_ShowNum(140, 135, (int)E53_SC2_Data.Temperature, 5, 16); LCD_ShowString(10, 160, 200, 16, 16, \"Acce_X:\"); LCD_ShowNum(140, 160, (int)E53_SC2_Data.Accel[0], 5, 16); LCD_ShowString(10, 185, 200, 16, 16, \"Acce_Y:\"); LCD_ShowNum(140, 185, (int)E53_SC2_Data.Accel[1], 5, 16); LCD_ShowString(10, 210, 200, 16, 16, \"Acce_Z:\"); LCD_ShowNum(140, 210, (int)E53_SC2_Data.Accel[2], 5, 16); osal_task_sleep(2*1000); } return 0; }\n5. Length: 730 tokens, Text: void CALENDAR_Init( CALENDAR_InfoStruct *pInfoStruct ) { RTC_DateTypeDef sdatestructure; RTC_TimeTypeDef stimestructure; RtcHandle.Instance = RTC; RtcHandle.Init.HourFormat = RTC_HOURFORMAT_24; RtcHandle.Init.AsynchPrediv = RTC_ASYNCH_PREDIV; RtcHandle.Init.SynchPrediv = RTC_SYNCH_PREDIV; RtcHandle.Init.OutPut = RTC_OUTPUT_DISABLE; RtcHandle.Init.OutPutPolarity = RTC_OUTPUT_POLARITY_HIGH; RtcHandle.Init.OutPutType = RTC_OUTPUT_TYPE_OPENDRAIN; __HAL_RTC_RESET_HANDLE_STATE(&RtcHandle); if (HAL_RTC_Init(&RtcHandle) != HAL_OK) { Error_Handler(); } HAL_GPIO_TogglePin(GPIOD, GPIO_PIN_13); if( pInfoStruct != NULL ) { sdatestructure.Year = pInfoStruct->year; sdatestructure.Month = pInfoStruct->month; sdatestructure.Date = pInfoStruct->day; sdatestructure.WeekDay = pInfoStruct->weekday; stimestructure.Hours = pInfoStruct->hours; stimestructure.Minutes = pInfoStruct->minutes; stimestructure.Seconds = pInfoStruct->seconds; stimestructure.SubSeconds = 0; stimestructure.SecondFraction = 0; stimestructure.TimeFormat = RTC_HOURFORMAT12_AM; stimestructure.DayLightSaving = RTC_DAYLIGHTSAVING_NONE; stimestructure.StoreOperation = RTC_STOREOPERATION_RESET; configure_calendar(&sdatestructure, &stimestructure); } else { if (HAL_RTCEx_BKUPRead(&RtcHandle, RTC_BKP_DR1) != 0x32F2) { sdatestructure.Year = 0x18; sdatestructure.Month = RTC_MONTH_MAY; sdatestructure.Date = 0x12; sdatestructure.WeekDay = RTC_WEEKDAY_SATURDAY; stimestructure.Hours = 0x02; stimestructure.Minutes = 0x00; stimestructure.Seconds = 0x00; stimestructure.SubSeconds = 0; stimestructure.SecondFraction = 0; stimestructure.TimeFormat = RTC_HOURFORMAT12_AM; stimestructure.DayLightSaving = RTC_DAYLIGHTSAVING_NONE; stimestructure.StoreOperation = RTC_STOREOPERATION_RESET; configure_calendar(&sdatestructure, &stimestructure); } else { if (__HAL_RCC_GET_FLAG(RCC_FLAG_PORRST) != RESET) { HAL_GPIO_TogglePin(GPIOD, GPIO_PIN_12); } if (__HAL_RCC_GET_FLAG(RCC_FLAG_PINRST) != RESET) { HAL_GPIO_TogglePin(GPIOD, GPIO_PIN_13); } __HAL_RCC_CLEAR_RESET_FLAGS(); } } }\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"sequences with simular length to have minimal padding","metadata":{}},{"cell_type":"code","source":"def analyze_sequence_lengths(dataset, tokenizer, name=\"Dataset\"):\n    \"\"\"\n    Compute average and longest sequence lengths for input_ids and labels in a tokenized dataset.\n\n    Args:\n        dataset: Hugging Face Dataset object with tokenized 'input_ids' and 'labels'\n        tokenizer: The tokenizer used (e.g., RobertaTokenizer for CodeT5)\n        name: Name of the dataset (e.g., 'Train', 'Validation') for printing\n    \"\"\"\n    # Extract sequence lengths for input_ids and labels\n    input_lengths = [len(seq) for seq in dataset['input_ids']]\n    label_lengths = [len(seq) for seq in dataset['labels']]\n\n    # Compute average and max lengths\n    avg_input_length = np.mean(input_lengths)\n    max_input_length = np.max(input_lengths)\n    avg_label_length = np.mean(label_lengths)\n    max_label_length = np.max(label_lengths)\n\n    # Find indices of the longest sequences\n    max_input_idx = np.argmax(input_lengths)\n    max_label_idx = np.argmax(label_lengths)\n\n    # Decode the longest sequences for inspection\n    longest_input_text = tokenizer.decode(dataset['input_ids'][max_input_idx], skip_special_tokens=True)\n    longest_label_text = tokenizer.decode(dataset['labels'][max_label_idx], skip_special_tokens=True)\n\n    # Print results\n    print(f\"\\n{name} Sequence Length Analysis:\")\n    print(f\"Input Sequences (input_ids):\")\n    print(f\"  Average Length: {avg_input_length:.2f} tokens\")\n    print(f\"  Max Length: {max_input_length} tokens\")\n    print(f\"  Longest Input Text: {longest_input_text}\")\n    print(f\"Label Sequences (labels):\")\n    print(f\"  Average Length: {avg_label_length:.2f} tokens\")\n    print(f\"  Max Length: {max_label_length} tokens\")\n    print(f\"  Longest Label Text: {longest_label_text}\")\n\n# Analyze all datasets\nanalyze_sequence_lengths(train_dataset, tokenizer, name=\"Train\")\nanalyze_sequence_lengths(val_dataset, tokenizer, name=\"Validation\")\nanalyze_sequence_lengths(test_dataset, tokenizer, name=\"Test\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dxbyhd0lRrFg","outputId":"efed8a1c-915e-4f8d-f377-7447d205297a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_sequences_by_length(dataset, tokenizer, max_input_tokens=32, max_target_tokens=64):\n    original_size = len(dataset)\n    filtered_dataset = dataset.filter(\n        lambda x: len(tokenizer(x['input_text'], truncation=False, padding=False)['input_ids']) <= max_input_tokens and\n                  len(tokenizer(x['target_text'], truncation=False, padding=False)['input_ids']) <= max_target_tokens,\n        desc=\"Filtering sequences\"\n    )\n    filtered_size = len(filtered_dataset)\n    print(f\"Original dataset size: {original_size}\")\n    print(f\"Filtered dataset size: {filtered_size}\")\n    print(f\"Removed {original_size - filtered_size} sequences\")\n    return filtered_dataset","metadata":{"id":"_bZOVliBnZmF","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:55.185801Z","iopub.execute_input":"2025-05-12T09:06:55.186027Z","iopub.status.idle":"2025-05-12T09:06:55.191073Z","shell.execute_reply.started":"2025-05-12T09:06:55.186011Z","shell.execute_reply":"2025-05-12T09:06:55.190261Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"filter_sequences_by_length(train_dataset,tokenizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["10d0a7e61ee64a48897168b41ee228e0","9d2a8094a8024ef5a900c9921467671f","ba7ed9c5f5e8414583b249a58108a4a2","3c36eb8707c642739bf519c11d235dd5","b7c1cc7474f54d579d44e5499b9215b6","9fa323a4d0554695b92f5b9c0f7c6931","7a059802ece6483bace57d6e5d63242c","d0dde1f6acdc46a2af02c584de742732","f1a1c8305b944a33b57a464d1c4bc9e5","aa2c4c2647bc4a74a82e3484d76ccd59","bee8d1ea0abf47be95e625e98a80cc40"]},"id":"p7Q4tocFnhuj","outputId":"5d2acb6f-b07c-44d8-dc67-5a76a6ea79f5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset=filter_sequences_by_length(train_dataset,tokenizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["ec03c8e9d5554c35927e78a3a64a0583","1460dd4a7ab5466a921ac9deb74696e4","e1d2d47293bf40cbaeb779c580cd67a4","b523d64c465b44eca3f7670b31bc5b47","f3253e6a9e85409783f2cd94a53dad49","19c1c03628054e88991902cb65c82678","807a0ae62f3f4bd1a75b7ee68e527f6e","24a15569409044b99c194d3967d09efb","f90f814545404c55a02dfcc18f13ad75","f41c8fc2ea154d6a9c3e9607d4b1648b","22b96d9aa254422d9cf3ecd94a9c5d9b"]},"id":"9kUPz7Z3oSUK","outputId":"ce6b05c7-6a3a-4215-8486-b7deb579b06e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:55.191736Z","iopub.execute_input":"2025-05-12T09:06:55.192160Z","iopub.status.idle":"2025-05-12T09:06:57.099529Z","shell.execute_reply.started":"2025-05-12T09:06:55.192133Z","shell.execute_reply":"2025-05-12T09:06:57.098850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filtering sequences:   0%|          | 0/3847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd656c41ce8a437b80e872dc03aeb0d5"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Original dataset size: 3847\nFiltered dataset size: 3104\nRemoved 743 sequences\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"val_dataset=filter_sequences_by_length(val_dataset,tokenizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["3e6ba82cb82543b1a19c43682b99c416","bf625ec286764992b0b94b14745de223","96c5c6de9eaa419e912fd247a32afb1f","5f718f84d9644d51b05c58135c427472","65b7539fec4543a3bb4763e96bd73788","f2b67c78f4b749b187cf89950d8e772b","9d0456aa33004ef6b984a7ed2b905e58","048242300a354ce88921f9037ce43e5d","fdc72e0a342b4948a759f67339ba554b","7019aede0a5b4b30838f36c2ab3f00fd","052b1f2e00cb4b98b077d4c5b88c5061"]},"id":"bdAfzDWmn2_B","outputId":"200bae77-66c7-4f95-e97f-c3a41f6bad06","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:57.100441Z","iopub.execute_input":"2025-05-12T09:06:57.100658Z","iopub.status.idle":"2025-05-12T09:06:57.823100Z","shell.execute_reply.started":"2025-05-12T09:06:57.100641Z","shell.execute_reply":"2025-05-12T09:06:57.822349Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filtering sequences:   0%|          | 0/481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e942b1a0844a41bf82feb5acb3a5b1"}},"metadata":{}},{"name":"stdout","text":"Original dataset size: 481\nFiltered dataset size: 357\nRemoved 124 sequences\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"test_dataset=filter_sequences_by_length(test_dataset,tokenizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["487757a1dd614fd8815531eb671e3ec0","075ee49c6f9a421284437ed28fb2af87","cb1748a9592a4eb784ae46bff04f65e7","e0f310d2a8624949bd2920320cf05932","1cfd884ea49f4eb8890c57b45a245215","d1259ff04bd44dc6a62e4c553b40c6b6","855d1498e20d418d977cb4cf03f61a9d","b8b060ef0f2f44e99c8cc277f40c6740","5b91c919751240debedf79c4a5712ddd","322a0853c78644b390dea9e3b7b11375","6b041ec2772e464db31873ad582b6eb7"]},"id":"oO2EqYjpoFVS","outputId":"1f65a510-55d1-4ccb-a6f6-700e1b2dcf1e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:57.823864Z","iopub.execute_input":"2025-05-12T09:06:57.824067Z","iopub.status.idle":"2025-05-12T09:06:58.535769Z","shell.execute_reply.started":"2025-05-12T09:06:57.824051Z","shell.execute_reply":"2025-05-12T09:06:58.535205Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filtering sequences:   0%|          | 0/481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e084c6ffae54766b090ad309c7c280f"}},"metadata":{}},{"name":"stdout","text":"Original dataset size: 481\nFiltered dataset size: 378\nRemoved 103 sequences\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Add a column for sequence length to the dataset\ndef add_sequence_length(example, tokenizer):\n    input_tokens = tokenizer(example['input_text'], truncation=False, padding=False)['input_ids']\n    target_tokens = tokenizer(example['target_text'], truncation=False, padding=False)['input_ids']\n    return {\n        'input_length': len(input_tokens),\n        'target_length': len(target_tokens)\n    }\n\n# Apply length calculation to datasets\ntrain_dataset = train_dataset.map(lambda x: add_sequence_length(x, tokenizer))\nval_dataset = val_dataset.map(lambda x: add_sequence_length(x, tokenizer))\ntest_dataset = test_dataset.map(lambda x: add_sequence_length(x, tokenizer))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["e058af0bfa17407d85eb0d37ae7bb511","6d12eaacccde445997cf6ea35b4b9f38","b6cb4cf0bdd14fdaae8e543c87894f60","3b71ab270f8d45f5975977fb70695624","d960954d3d014d67be1c337f4c4d53bf","86b72047faa946db8332cbc294b2bb0d","f882545ac2234cc9b869025b91434530","62b9d7c0a9b044c1ae38889c5768e1d2","42f8c388a77448388e3917e97529199c","67d3791aabf14d59b29c7542704baebb","f51de7b8cabc4b53b98890fc589efa3f","8010b53f260b4d4f9feb9eb71ac64d33","a6a3fb68d3ad4063a3b5678c5eaf8ff9","0255c2b51e44441ca30483504f8f777b","4a3097f49b2f44eea48f3d195a374fab","d8f4a8790997402d9558daf1a6e99108","bcb82598103344baac44a2ed57222126","f23e28cf266d49a59556f361dde70d97","45220877402b4ab3b8da957a2d0594cf","151a276a84164f60a02aa0a5f6f67f51","5ee2f9bb03144c859dbcd74c8549c1b3","9e6ec6d0a97a48d18084ba6c3732d568","f644ce2cf5fb464585bf261b91d46605","3578fd3b55044de08cad3850ed68c702","5f792ce1f12841018e473909220152dd","7419fc1b5475458cbe2e98e1897908fc","ecf1bf5224c943dd9b8cbebb4779e697","9f273325e3494c2cb56d0027fd0b9487","f2f966ccead045ee9020f52d01238552","df0b5731ac5342be90d1ce1e5e4ff104","8fe08a0df453474e9e62166fd9f4d860","22172ebcc7e046e294fd2633ff4ad827","00adf9dccc3642d48ac32e2fbd652441"]},"id":"dkcPnLbyGyIw","outputId":"1f3b0d2c-f1d6-401e-a4c8-888c1475683e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:06:58.536466Z","iopub.execute_input":"2025-05-12T09:06:58.536665Z","iopub.status.idle":"2025-05-12T09:07:02.395870Z","shell.execute_reply.started":"2025-05-12T09:06:58.536646Z","shell.execute_reply":"2025-05-12T09:07:02.395032Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3104 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e20c83d1dc441878997bea70b78cd4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43992bae9d9c465196cb1f6c34ee5c1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/378 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9c2521366c4df4b1e8095040122ef3"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Collect input lengths for sorting in collator\ntrain_input_lengths = [example['input_length'] for example in train_dataset]\nval_input_lengths = [example['input_length'] for example in val_dataset]\ntest_input_lengths = [example['input_length'] for example in test_dataset]","metadata":{"id":"aXeR726AN_Hh","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:02.396661Z","iopub.execute_input":"2025-05-12T09:07:02.396928Z","iopub.status.idle":"2025-05-12T09:07:02.547346Z","shell.execute_reply.started":"2025-05-12T09:07:02.396901Z","shell.execute_reply":"2025-05-12T09:07:02.546631Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Step 6: Tokenize the Dataset\ndef tokenize_function(examples):\n    model_inputs = tokenizer(\n        examples['input_text'],\n        #max_length=32,  # Reduced for Colab memory constraints\n        truncation=True,\n        padding='longest'\n    )\n    labels = tokenizer(\n        examples['target_text'],\n        #max_length=64,\n        truncation=True,\n        padding='longest'\n    )\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["b6667efb53334034bed0746af98d09b6","48c4752dcc3f484695abc96a07548d54","536fa1ee22fe4e70a39cce0a0dda2ab4","87f6e17763dd4a54808b500897b55121","69cdddf5640e40659fedfda0868085e1","933747f857114790b0f6ed148c2d929f","1b089ac25e3c432693a156eb9e73c3ec","4e6ea311d5a84d4699a5634832c2caef","ca41222597124af4b74aec7a0b517606","ce567ab079d24917b6f7f10517804915","1fcd70c64b954c4f860f4f1516f2b94c","82f6b5a079674d53a818fe12f0fc1417","2e5bd3b3e96d429bbaae37f1505b5b8b","de61025093b9432da804ff9dfcc033db","406462f234cd42ab87ca5a6270b85ee8","db01ba0c2cd244628764e94e0d91c4d7","3d5e9e85f0eb4cc4bff183b9174f578a","be9baa0a8279483f84d4d109bed26442","7b7a4ed97add4165892df3bdd8bc2dc6","546ae15f7b48490b95445e81fd1824aa","ffa4d5b1cb464daa89b64cf996175c9c","91a3fdefa3a54b5d9449c519a2254053","fff5d79be9f54903b98252d5f987cd87","2d11e4f9846144d6b0bf16ea99945b13","5771daa09c3a4b5a869e663349a1336c","11604d6e8f8e44c8ba9a11dad31375ac","629fc84932c84dd99792bc741f4fff30","ef5e772224e64d88afd46757118b8ef8","8d735abd86934e6bbb56dbc1bbf809c8","0f1b9f5a29b24f15ad761aab9e5c2c5d","8ca609d8e8154186bd4b2158ecb3e402","c08eababe8254ce581c59d3d61638d60","8de4363f820f499aa49735450fea9e3c"]},"id":"NZzpVEZNDMrZ","outputId":"7809f0d7-e9e3-4011-94cf-92a68da24daf","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:02.548177Z","iopub.execute_input":"2025-05-12T09:07:02.548422Z","iopub.status.idle":"2025-05-12T09:07:05.168399Z","shell.execute_reply.started":"2025-05-12T09:07:02.548405Z","shell.execute_reply":"2025-05-12T09:07:05.167674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3104 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3beb3cc2292541c78187edaa81fc7b1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2e2f8cc8b8408ab90339bbe165c40d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/378 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b03d6828859422d85ff671b96f99e5a"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Sort datasets by input_length\ntrain_dataset = train_dataset.sort('input_length')\nval_dataset = val_dataset.sort('input_length')\ntest_dataset = test_dataset.sort('input_length')","metadata":{"id":"WkFS-Z0DKwZ5","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:05.169145Z","iopub.execute_input":"2025-05-12T09:07:05.169372Z","iopub.status.idle":"2025-05-12T09:07:05.188480Z","shell.execute_reply.started":"2025-05-12T09:07:05.169350Z","shell.execute_reply":"2025-05-12T09:07:05.187670Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Debug: Verify dataset keys\nprint(\"Train dataset keys:\", train_dataset[0].keys())\nprint(\"Val dataset keys:\", val_dataset[0].keys())\nprint(\"Test dataset keys:\", test_dataset[0].keys())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqVrTHt4LIMw","outputId":"489e3415-2eb2-4284-f9c4-f996c4c4b58f","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T00:31:04.681513Z","iopub.execute_input":"2025-05-12T00:31:04.682103Z","iopub.status.idle":"2025-05-12T00:31:04.688345Z","shell.execute_reply.started":"2025-05-12T00:31:04.682076Z","shell.execute_reply":"2025-05-12T00:31:04.687498Z"}},"outputs":[{"name":"stdout","text":"Train dataset keys: dict_keys(['input_text', 'target_text', '__index_level_0__', 'input_length', 'target_length', 'input_ids', 'attention_mask', 'labels'])\nVal dataset keys: dict_keys(['input_text', 'target_text', '__index_level_0__', 'input_length', 'target_length', 'input_ids', 'attention_mask', 'labels'])\nTest dataset keys: dict_keys(['input_text', 'target_text', '__index_level_0__', 'input_length', 'target_length', 'input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Set dataset format for PyTorch, retaining input_text and target_text\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'input_text', 'target_text'])","metadata":{"id":"voYalMGADMuR","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:05.189338Z","iopub.execute_input":"2025-05-12T09:07:05.189543Z","iopub.status.idle":"2025-05-12T09:07:05.196688Z","shell.execute_reply.started":"2025-05-12T09:07:05.189529Z","shell.execute_reply":"2025-05-12T09:07:05.196070Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Step 7: Configure LoRA\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    #target_modules=[\"q\", \"v\", \"k\", \"o\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\"\n)\nmodel = get_peft_model(model, lora_config)","metadata":{"id":"pwKP4bRuDMxE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Data Collator\nclass LengthSortedDataCollator(DataCollatorForSeq2Seq):\n    def __init__(self, tokenizer, model, max_batch_size, input_lengths, pad_to_multiple_of=8):\n        super().__init__(tokenizer=tokenizer, model=model, pad_to_multiple_of=pad_to_multiple_of)\n        self.max_batch_size = max_batch_size\n        self.input_lengths = input_lengths  # Store input lengths\n\n    def __call__(self, features):\n        # Debug: Print available keys in features\n        #print(\"Collator features keys:\", [f.keys() for f in features])\n\n        # Pair features with their corresponding input lengths\n        # Since dataset is pre-sorted, we can use the order of input_lengths\n        indexed_features = list(zip(self.input_lengths[:len(features)], features))\n\n        # Sort by input_length (descending)\n        indexed_features = sorted(indexed_features, key=lambda x: x[0], reverse=True)\n\n        # Extract sorted features\n        sorted_features = [f for _, f in indexed_features]\n\n        # Group into batches of max_batch_size\n        batched_features = []\n        for i in range(0, len(sorted_features), self.max_batch_size):\n            batch = sorted_features[i:i + self.max_batch_size]\n            batched_features.append(batch)\n\n        # Process each batch using the parent class's collation logic\n        final_batches = []\n        for batch in batched_features:\n            batch = super().__call__(batch)\n            final_batches.append(batch)\n\n        # Return the first batch (Trainer expects a single batch at a time)\n        return final_batches[0]","metadata":{"id":"PY5U3ZAaG54_","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:05.197451Z","iopub.execute_input":"2025-05-12T09:07:05.197786Z","iopub.status.idle":"2025-05-12T09:07:05.214259Z","shell.execute_reply.started":"2025-05-12T09:07:05.197750Z","shell.execute_reply":"2025-05-12T09:07:05.213440Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Step 8: Set Up Data Collator\ndata_collator = LengthSortedDataCollator(\n    tokenizer=tokenizer,\n    model=model,\n    max_batch_size=2,  # Match per_device_train_batch_size\n    input_lengths=train_input_lengths,  # Pass train input lengths\n    pad_to_multiple_of=8\n)","metadata":{"id":"1p8Z8KlbDMzw","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:05.215125Z","iopub.execute_input":"2025-05-12T09:07:05.215436Z","iopub.status.idle":"2025-05-12T09:07:05.231563Z","shell.execute_reply.started":"2025-05-12T09:07:05.215416Z","shell.execute_reply":"2025-05-12T09:07:05.230835Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Step 9: Configure Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./stm32_codet5\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,  # Match max_batch_size in collator\n    per_device_eval_batch_size=2,\n    num_train_epochs=7,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=1,\n    report_to=\"none\",\n    run_name=\"stm32_codet5_run\",\n    fp16=True,\n)","metadata":{"id":"c02_TM_fDM25","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Xai: metrics / attention score / how each hyperparameters affects loss.. (not actual training)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom transformers import Trainer, TrainingArguments, T5ForConditionalGeneration\nfrom peft import LoraConfig, get_peft_model\nfrom itertools import product\n\n# Default hyperparameter values (based on your original code)\ndefault_r = 32\ndefault_target_modules = [\"q\", \"v\", \"k\", \"o\"]\ndefault_learning_rate = 2e-5\ndefault_batch_size = 2\n\n# Hyperparameter ranges to test\nhyperparam_ranges = {\n    'r': [8, 16, 32, 64],\n    'target_modules': [\n        [\"q\", \"v\"],\n        [\"q\", \"v\", \"k\", \"o\"],\n        [\"q\", \"v\", \"k\", \"o\", \"wi\", \"wo\"]\n    ],\n    'learning_rate': [1e-5, 2e-5, 5e-5],\n    'per_device_train_batch_size': [2, 4]\n}\n\n# Function to configure and train model for a single hyperparameter setting\ndef train_with_hyperparam(hyperparam_name, hyperparam_value, output_dir):\n    # Configure LoRA with the tested hyperparameter\n    lora_config_params = {\n        'r': hyperparam_value if hyperparam_name == 'r' else default_r,\n        'lora_alpha': 32,\n        'target_modules': hyperparam_value if hyperparam_name == 'target_modules' else default_target_modules,\n        'lora_dropout': 0.1,\n        'bias': \"none\",\n        'task_type': \"SEQ_2_SEQ_LM\"\n    }\n    lora_config = LoraConfig(**lora_config_params)\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    model = get_peft_model(model, lora_config)\n\n    # Configure batch size\n    batch_size = hyperparam_value if hyperparam_name == 'per_device_train_batch_size' else default_batch_size\n\n    # Update data collator with batch size\n    data_collator = LengthSortedDataCollator(\n        tokenizer=tokenizer,\n        model=model,\n        max_batch_size=batch_size,\n        input_lengths=train_input_lengths,\n        pad_to_multiple_of=8\n    )\n\n    # Configure training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        learning_rate=hyperparam_value if hyperparam_name == 'learning_rate' else default_learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=2,\n        weight_decay=0.01,\n        logging_dir=f'{output_dir}/logs',\n        logging_steps=10,\n        save_total_limit=1,\n        report_to=\"none\",\n        run_name=f\"stm32_codet5_{hyperparam_name}_{str(hyperparam_value).replace('.', '_')}\",\n        fp16=True,\n    )\n\n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=data_collator,\n    )\n\n    # Train and evaluate\n    trainer.train()\n    eval_results = trainer.evaluate()\n\n    # Save loss logs\n    log_history = trainer.state.log_history\n    with open(f\"{output_dir}/loss_log.json\", \"w\") as f:\n        json.dump(log_history, f, indent=4)\n\n    print(f\"Completed: {hyperparam_name}={hyperparam_value}, Validation Loss: {eval_results['eval_loss']}\")\n\n# Run experiments for each hyperparameter\nfor hyperparam_name, values in hyperparam_ranges.items():\n    print(f\"\\nTesting hyperparameter: {hyperparam_name}\")\n    for value in values:\n        output_dir = f\"./stm32_codet5_{hyperparam_name}_{str(value).replace('.', '_')}\"\n        try:\n            train_with_hyperparam(hyperparam_name, value, output_dir)\n        except Exception as e:\n            print(f\"Failed for {hyperparam_name}={value}: {e}\")\n\n# After running, analyze logs\nprint(\"\\nTo analyze results, check the 'loss_log.json' files in each output directory.\")\nprint(\"Each file contains training and validation loss per logging step/epoch.\")\nprint(\"Use the following script to visualize loss trends:\")\n\n# Script to analyze and visualize loss trends\nimport matplotlib.pyplot as plt\nimport json\nimport os\n\ndef plot_loss_trends(hyperparam_name, values, base_dir=\"./\"):\n    plt.figure(figsize=(10, 6))\n    for value in values:\n        output_dir = f\"{base_dir}/stm32_codet5_{hyperparam_name}_{str(value).replace('.', '_')}\"\n        log_file = f\"{output_dir}/loss_log.json\"\n        if os.path.exists(log_file):\n            with open(log_file, \"r\") as f:\n                logs = json.load(f)\n            train_loss = [(log['step'], log['loss']) for log in logs if 'loss' in log]\n            eval_loss = [(log['epoch'], log['eval_loss']) for log in logs if 'eval_loss' in log]\n            \n            if train_loss:\n                steps, losses = zip(*train_loss)\n                plt.plot(steps, losses, label=f\"{hyperparam_name}={value} (Train)\")\n            if eval_loss:\n                epochs, losses = zip(*eval_loss)\n                plt.plot(epochs, losses, '--', label=f\"{hyperparam_name}={value} (Val)\")\n    \n    plt.title(f\"Loss Trends for {hyperparam_name}\")\n    plt.xlabel(\"Steps (Train) / Epochs (Val)\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f\"{base_dir}/loss_trends_{hyperparam_name}.png\")\n    plt.close()\n\n# Generate plots for each hyperparameter\nfor hyperparam_name, values in hyperparam_ranges.items():\n    plot_loss_trends(hyperparam_name, values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:07:05.232339Z","iopub.execute_input":"2025-05-12T09:07:05.232582Z"}},"outputs":[{"name":"stdout","text":"\nTesting hyperparameter: r\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1465' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1465/3104 04:49 < 05:24, 5.05 it/s, Epoch 0.94/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"execute for actual training","metadata":{}},{"cell_type":"code","source":"# Step 10: Initialize and Train the Model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"Li4z4X3oDM55","outputId":"e4b89a53-64cc-4200-a4a1-11416ba1ba36","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T20:20:45.801051Z","iopub.execute_input":"2025-05-11T20:20:45.801829Z","iopub.status.idle":"2025-05-11T20:49:08.308280Z","shell.execute_reply.started":"2025-05-11T20:20:45.801804Z","shell.execute_reply":"2025-05-11T20:49:08.307553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10864' max='10864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10864/10864 28:21, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.938400</td>\n      <td>0.820758</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.818300</td>\n      <td>0.744302</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.861400</td>\n      <td>0.705085</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.725000</td>\n      <td>0.683939</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.717100</td>\n      <td>0.670061</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.546800</td>\n      <td>0.662210</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.746300</td>\n      <td>0.659903</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10864, training_loss=0.8474417003818043, metrics={'train_runtime': 1702.0133, 'train_samples_per_second': 12.766, 'train_steps_per_second': 6.383, 'total_flos': 841728497025024.0, 'train_loss': 0.8474417003818043, 'epoch': 7.0})"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"test prompts different prompts (short, long)","metadata":{}},{"cell_type":"code","source":"# Define prompts\nprompts = {\n    'concise': lambda comment: f\"Generate STM32 code for: {comment}\",\n    'detailed': lambda comment: f\"As an STM32 programming expert, generate the STM32 code that implements the functionality described: {comment}\",\n    'structured': lambda comment: f\"[Task: Generate STM32 Code]\\n[Comment]: {comment}\\n[Output]: STM32 code\",\n    'action_oriented': lambda comment: f\"Write STM32 code to: {comment}\"\n}\n\n# Test comments\ntest_comments = [\n    \"Function call for delay of 500ms\",\n    \"Toggle LED9 on the STM32 board\",\n    \"Reset the LED pin to turn it off\",\n    \"Initialize UART4 peripheral\"\n]\n\n# Verify model and tokenizer are in memory\ntry:\n    assert 'model' in globals() and 'tokenizer' in globals(), \"Model or tokenizer not found in memory\"\n    print(\"Model and tokenizer found in memory!\")\nexcept AssertionError:\n    raise Exception(\"Model or tokenizer not found. Ensure you run this in the same session as training or load the saved model.\")\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Generate outputs for each prompt and comment\nfor prompt_name, prompt_fn in prompts.items():\n    print(f\"\\n=== Outputs for {prompt_name} prompt ===\\n\")\n    for comment in test_comments:\n        input_text = prompt_fn(comment)\n        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n        outputs = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Comment: {comment}\")\n        print(f\"Generated Code: {generated_code}\\n\")\n\n# Optional: Reload dataset if df is not in memory (uncomment if needed)\n\"\"\"\nimport pandas as pd\ndf = pd.read_csv('stm32_code_comments_dataset_scored.csv')\ndf = df[df['relevance_score'] > 60]\nprint(f\"Filtered dataset size: {len(df)}\")\nprint(df[['comment', 'code']].head())\n\"\"\"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"C6DgXfigPsUI","outputId":"a5976fef-9fb9-4efd-916f-0c56295bf840","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:05:28.122154Z","iopub.execute_input":"2025-05-11T19:05:28.123229Z","iopub.status.idle":"2025-05-11T19:05:34.108237Z","shell.execute_reply.started":"2025-05-11T19:05:28.123186Z","shell.execute_reply":"2025-05-11T19:05:34.107513Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer found in memory!\n\n=== Outputs for concise prompt ===\n\nComment: Function call for delay of 500ms\nGenerated Code: HAL_Delay(500);\n\nComment: Toggle LED9 on the STM32 board\nGenerated Code: HAL_LED9_Toggle(LED9);\n\nComment: Reset the LED pin to turn it off\nGenerated Code: HAL_LED_Pin_Reset();\n\nComment: Initialize UART4 peripheral\nGenerated Code: HAL_UART4_Init();\n\n\n=== Outputs for detailed prompt ===\n\nComment: Function call for delay of 500ms\nGenerated Code: HAL_RCC_Delay(500);\n\nComment: Toggle LED9 on the STM32 board\nGenerated Code: HAL_LED9_Toggle(LED9);\n\nComment: Reset the LED pin to turn it off\nGenerated Code: HAL_LED_Pin_Reset(LED_Pin);\n\nComment: Initialize UART4 peripheral\nGenerated Code: HAL_UART4_Init();\n\n\n=== Outputs for structured prompt ===\n\nComment: Function call for delay of 500ms\nGenerated Code: HAL_HAL_HAL_HAL_DELAY(500ms);\n\nComment: Toggle LED9 on the STM32 board\nGenerated Code: HAL_LED9_Toggle(LED9);\n\nComment: Reset the LED pin to turn it off\nGenerated Code: HAL_LED_Pin_Off(LED_Pin);\n\nComment: Initialize UART4 peripheral\nGenerated Code: HAL_UART4_Peripheral\n_Init();\n\n\n=== Outputs for action_oriented prompt ===\n\nComment: Function call for delay of 500ms\nGenerated Code: HAL_Delay(500);\n\nComment: Toggle LED9 on the STM32 board\nGenerated Code: HAL_LED9_Toggle(LED9);\n\nComment: Reset the LED pin to turn it off\nGenerated Code: HAL_LED_Pin_Reset(LED_PIN);\n\nComment: Initialize UART4 peripheral\nGenerated Code: HAL_UART4_Init();\n\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'\\nimport pandas as pd\\ndf = pd.read_csv(\\'stm32_code_comments_dataset_scored.csv\\')\\ndf = df[df[\\'relevance_score\\'] > 60]\\nprint(f\"Filtered dataset size: {len(df)}\")\\nprint(df[[\\'comment\\', \\'code\\']].head())\\n'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import random","metadata":{"id":"NuLFty6gSYCw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_random_tokens(dataset, tokenizer, num_examples=5):\n    random_indices = random.sample(range(len(dataset)), num_examples)\n    print(f\"\\nDisplaying tokens for {num_examples} random sequences:\\n\")\n    for i, idx in enumerate(random_indices):\n        input_ids = dataset[idx]['input_ids'].tolist()\n        labels = dataset[idx]['labels'].tolist()\n        input_tokens = [tokenizer.convert_ids_to_tokens(id) for id in input_ids]\n        label_tokens = [tokenizer.convert_ids_to_tokens(id) for id in labels]\n        print(f\"Example {i+1} (Index: {idx}):\")\n        print(f\"Input Tokens: {input_tokens}\")\n        print(f\"Label Tokens: {label_tokens}\")\n        print(\"-\" * 80)\n\ndisplay_random_tokens(train_dataset, tokenizer, num_examples=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPTE6S2eSWIy","outputId":"c7521513-bbb6-42ef-ebb8-2660018a6589","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"test","metadata":{}},{"cell_type":"code","source":"pip install captum","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHqzvThTRlEI","outputId":"5e8142a9-99a6-48fc-fbb0-d7c452e47db8","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:00:48.742447Z","iopub.execute_input":"2025-05-11T21:00:48.743181Z","iopub.status.idle":"2025-05-11T21:00:52.456805Z","shell.execute_reply.started":"2025-05-11T21:00:48.743156Z","shell.execute_reply":"2025-05-11T21:00:52.455872Z"}},"outputs":[{"name":"stdout","text":"Collecting captum\n  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.7.5)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\nRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum) (2024.2.0)\nDownloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.8.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom captum.attr import IntegratedGradients\nimport shap\nfrom nltk.translate.bleu_score import sentence_bleu\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import T5ForConditionalGeneration, RobertaTokenizer\nfrom datasets import Dataset\n","metadata":{"id":"G3qIFfohPxQl","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:00:52.458404Z","iopub.execute_input":"2025-05-11T21:00:52.458640Z","iopub.status.idle":"2025-05-11T21:00:55.173876Z","shell.execute_reply.started":"2025-05-11T21:00:52.458619Z","shell.execute_reply":"2025-05-11T21:00:55.173338Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = test_dataset[0]\ninput_text = sample['input_text']\ntarget_text = sample['target_text']  # or whatever the actual key name is\n\n# 1. Attention Visualization\ndef visualize_attention(input_text, output_text, model, tokenizer, layer_idx=0, head_idx=0):\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    outputs = model.generate(**inputs, max_length=64, output_attentions=True, return_dict_in_generate=True)\n    \n    # Extract cross-attention (decoder attending to encoder outputs) from the first layer and head\n    cross_attention = outputs.attentions[0][layer_idx][0][head_idx]  # [tgt_len, src_len]\n    \n    input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    output_tokens = tokenizer.convert_ids_to_tokens(outputs.sequences[0])\n    \n    # Plot heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(cross_attention.cpu().detach().numpy(), xticklabels=input_tokens, yticklabels=output_tokens, cmap=\"viridis\")\n    plt.title(f\"Cross-Attention (Layer {layer_idx}, Head {head_idx})\")\n    plt.xlabel(\"Input Tokens\")\n    plt.ylabel(\"Output Tokens\")\n    plt.savefig(\"attention_heatmap.png\")\n    plt.close()\n\n\nvisualize_attention(input_text, target_text, model, tokenizer, layer_idx=0, head_idx=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:06:19.813684Z","iopub.execute_input":"2025-05-11T19:06:19.814698Z","iopub.status.idle":"2025-05-11T19:06:20.106638Z","shell.execute_reply.started":"2025-05-11T19:06:19.814675Z","shell.execute_reply":"2025-05-11T19:06:20.105519Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2502593302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/2502593302.py\u001b[0m in \u001b[0;36mvisualize_attention\u001b[0;34m(input_text, output_text, model, tokenizer, layer_idx, head_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Extract cross-attention (decoder attending to encoder outputs) from the first layer and head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcross_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead_idx\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [tgt_len, src_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'GenerateEncoderDecoderOutput' object has no attribute 'attentions'"],"ename":"AttributeError","evalue":"'GenerateEncoderDecoderOutput' object has no attribute 'attentions'","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Input prompt\nprompt = \"[Instruction] Generate STM32 code for the following description: initialize GPIO for STM32\"\ninputs = tokenizer(prompt, return_tensors=\"pt\", max_length=32, truncation=True)\ninputs = {k: v.to(device) for k, v in inputs.items()}  # MOVE INPUTS TO DEVICE\n\nmodel = model.to(device)  # MOVE MODEL TO DEVICE\n\n# Forward pass with attention outputs\nwith torch.no_grad():\n    outputs = model(**inputs, output_attentions=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:06:24.821396Z","iopub.execute_input":"2025-05-11T19:06:24.821657Z","iopub.status.idle":"2025-05-11T19:06:24.878789Z","shell.execute_reply.started":"2025-05-11T19:06:24.821639Z","shell.execute_reply":"2025-05-11T19:06:24.877842Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/803404901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Forward pass with attention outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1786\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1740\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"],"ename":"ValueError","evalue":"You have to specify either decoder_input_ids or decoder_inputs_embeds","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\ndecoder_inputs = tokenizer(target_text, return_tensors=\"pt\")\n\ninputs = {k: v.to(device) for k, v in inputs.items()}\ndecoder_inputs = {k: v.to(device) for k, v in decoder_inputs.items()}\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:06:29.689373Z","iopub.execute_input":"2025-05-11T19:06:29.690084Z","iopub.status.idle":"2025-05-11T19:06:29.709245Z","shell.execute_reply.started":"2025-05-11T19:06:29.690060Z","shell.execute_reply":"2025-05-11T19:06:29.708575Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32100, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32100, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-11): 11 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32100, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-11): 11 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseActDense(\n                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): ReLU()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import torch\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration\nfrom nltk.translate.bleu_score import sentence_bleu\nimport re\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define 10 prompts and target STM32 code snippets\nprompts_targets = [\n    {\n        \"prompt\": \"Generate STM32 code to enable the clock for GPIO Port B.\",\n        \"target\": \"__HAL_RCC_GPIOB_CLK_ENABLE();\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to initialize a delay with 80 MHz system clock.\",\n        \"target\": \"delay_init(80);\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to transmit data over UART1 with HAL.\",\n        \"target\": \"HAL_UART_Transmit(&huart1, data, size, 1000);\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to initialize the system kernel.\",\n        \"target\": \"osKernelStart();\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to initialize a PID controller with float32.\",\n        \"target\": \"arm_pid_init_f32(&pid, 1);\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to enable the clock for GPIO Port A.\",\n        \"target\": \"__HAL_RCC_GPIOA_CLK_ENABLE();\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to configure GPIO Pin 5 as output.\",\n        \"target\": \"HAL_GPIO_Init(GPIOA, &GPIO_InitStruct);\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to toggle GPIO Pin 13 on Port C.\",\n        \"target\": \"HAL_GPIO_TogglePin(GPIOC, GPIO_PIN_13);\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to initialize the network stack.\",\n        \"target\": \"net_init();\"\n    },\n    {\n        \"prompt\": \"Generate STM32 code to start the ADC conversion.\",\n        \"target\": \"HAL_ADC_Start(&hadc1);\"\n    }\n]\n\ndef clean_tokens(tokens):\n    \"\"\"Remove special tokens and punctuation, keep meaningful code tokens.\"\"\"\n    # Define tokens to exclude (special tokens and punctuation)\n    exclude = {'<s>', '</s>', '(', ')', ';', '&', ','}\n    # Keep only alphanumeric tokens or underscores, exclude special/punctuation\n    return [t for t in tokens if t not in exclude and re.match(r'[\\w]+', t)]\n\ndef evaluate_token_accuracy(prompt, target, model, tokenizer):\n    \"\"\"Generate prediction and compute token-based accuracy ignoring order.\"\"\"\n    # Tokenize input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n    \n    # Generate prediction\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=64,\n        num_beams=5,\n        early_stopping=True\n    )\n    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Tokenize target and predicted text\n    target_tokens = tokenizer.tokenize(target)\n    predicted_tokens = tokenizer.tokenize(predicted_text)\n    \n    # Clean tokens (remove punctuation, special tokens)\n    target_tokens_clean = clean_tokens(target_tokens)\n    predicted_tokens_clean = clean_tokens(predicted_tokens)\n    \n    # Convert to sets to ignore order\n    target_set = set(target_tokens_clean)\n    predicted_set = set(predicted_tokens_clean)\n    \n    # Compute matching tokens\n    matching_tokens = target_set & predicted_set\n    num_matching = len(matching_tokens)\n    num_target = len(target_set) if len(target_set) > 0 else 1  # Avoid division by zero\n    \n    # Accuracy for this example\n    accuracy = num_matching / num_target\n    \n    return {\n        \"prompt\": prompt,\n        \"target\": target,\n        \"predicted\": predicted_text,\n        \"target_tokens\": target_tokens_clean,\n        \"predicted_tokens\": predicted_tokens_clean,\n        \"matching_tokens\": matching_tokens,\n        \"accuracy\": accuracy\n    }\n\n# Evaluate all prompts\nresults = []\nfor item in prompts_targets:\n    result = evaluate_token_accuracy(item[\"prompt\"], item[\"target\"], model, tokenizer)\n    results.append(result)\n\n# Compute overall accuracy\ntotal_accuracy = sum(r[\"accuracy\"] for r in results) / len(results)\n\n# Print results\nprint(\"STM32 Code Prediction Results:\")\nfor i, result in enumerate(results):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"Prompt: {result['prompt']}\")\n    print(f\"Target: {result['target']}\")\n    print(f\"Predicted: {result['predicted']}\")\n    print(f\"Target Tokens: {result['target_tokens']}\")\n    print(f\"Predicted Tokens: {result['predicted_tokens']}\")\n    print(f\"Matching Tokens: {result['matching_tokens']}\")\n    print(f\"Accuracy: {result['accuracy']:.4f}\")\n\nprint(f\"\\nOverall Token Accuracy: {total_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:26:22.542015Z","iopub.execute_input":"2025-05-11T21:26:22.542597Z","iopub.status.idle":"2025-05-11T21:26:26.279432Z","shell.execute_reply.started":"2025-05-11T21:26:22.542565Z","shell.execute_reply":"2025-05-11T21:26:26.278751Z"}},"outputs":[{"name":"stdout","text":"STM32 Code Prediction Results:\n\nExample 1:\nPrompt: Generate STM32 code to enable the clock for GPIO Port B.\nTarget: __HAL_RCC_GPIOB_CLK_ENABLE();\nPredicted: HAL_GPIO_Init(GPIO_PORT_B);\nTarget Tokens: ['__', 'H', 'AL', '_', 'R', 'CC', '_', 'G', 'PI', 'OB', '_', 'CL', 'K', '_', 'ENABLE']\nPredicted Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Init', 'G', 'PIO', '_', 'PORT', '_', 'B']\nMatching Tokens: {'H', 'G', 'AL', '_'}\nAccuracy: 0.3333\n\nExample 2:\nPrompt: Generate STM32 code to initialize a delay with 80 MHz system clock.\nTarget: delay_init(80);\nPredicted: HAL_Delay_Init(80);\nTarget Tokens: ['delay', '_', 'init', '80']\nPredicted Tokens: ['H', 'AL', '_', 'Delay', '_', 'Init', '80']\nMatching Tokens: {'80', '_'}\nAccuracy: 0.5000\n\nExample 3:\nPrompt: Generate STM32 code to transmit data over UART1 with HAL.\nTarget: HAL_UART_Transmit(&huart1, data, size, 1000);\nPredicted: HAL_UART1_Transmit(&huart1, 1);\nTarget Tokens: ['H', 'AL', '_', 'U', 'ART', '_', 'Trans', 'mit', 'h', 'u', 'art', '1', 'Ġdata', 'Ġsize', 'Ġ1000']\nPredicted Tokens: ['H', 'AL', '_', 'U', 'ART', '1', '_', 'Trans', 'mit', 'h', 'u', 'art', '1', 'Ġ1']\nMatching Tokens: {'1', 'u', '_', 'H', 'mit', 'art', 'ART', 'AL', 'h', 'U', 'Trans'}\nAccuracy: 0.7857\n\nExample 4:\nPrompt: Generate STM32 code to initialize the system kernel.\nTarget: osKernelStart();\nPredicted: HAL_SystemKernel_Init();\nTarget Tokens: ['os', 'Kernel', 'Start']\nPredicted Tokens: ['H', 'AL', '_', 'System', 'Kernel', '_', 'Init']\nMatching Tokens: {'Kernel'}\nAccuracy: 0.3333\n\nExample 5:\nPrompt: Generate STM32 code to initialize a PID controller with float32.\nTarget: arm_pid_init_f32(&pid, 1);\nPredicted: HAL_PID_Controller_Init();\nTarget Tokens: ['arm', '_', 'pid', '_', 'init', '_', 'f', '32', 'pid', 'Ġ1']\nPredicted Tokens: ['H', 'AL', '_', 'PID', '_', 'Controller', '_', 'Init']\nMatching Tokens: {'_'}\nAccuracy: 0.1429\n\nExample 6:\nPrompt: Generate STM32 code to enable the clock for GPIO Port A.\nTarget: __HAL_RCC_GPIOA_CLK_ENABLE();\nPredicted: HAL_GPIO_Init(GPIO_PORT_A);\nTarget Tokens: ['__', 'H', 'AL', '_', 'R', 'CC', '_', 'G', 'PIO', 'A', '_', 'CL', 'K', '_', 'ENABLE']\nPredicted Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Init', 'G', 'PIO', '_', 'PORT', '_', 'A']\nMatching Tokens: {'G', '_', 'H', 'A', 'PIO', 'AL'}\nAccuracy: 0.5000\n\nExample 7:\nPrompt: Generate STM32 code to configure GPIO Pin 5 as output.\nTarget: HAL_GPIO_Init(GPIOA, &GPIO_InitStruct);\nPredicted: HAL_GPIO_WritePin(GPIO_PORT, 5);\nTarget Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Init', 'G', 'PIO', 'A', 'Ġ&', 'G', 'PIO', '_', 'Init', 'Struct']\nPredicted Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Write', 'Pin', 'G', 'PIO', '_', 'PORT', 'Ġ5']\nMatching Tokens: {'G', '_', 'H', 'PIO', 'AL'}\nAccuracy: 0.5556\n\nExample 8:\nPrompt: Generate STM32 code to toggle GPIO Pin 13 on Port C.\nTarget: HAL_GPIO_TogglePin(GPIOC, GPIO_PIN_13);\nPredicted: HAL_GPIO_TogglePin(GPIO_Port, 13);\nTarget Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Toggle', 'Pin', 'G', 'PI', 'OC', 'ĠGPIO', '_', 'P', 'IN', '_', '13']\nPredicted Tokens: ['H', 'AL', '_', 'G', 'PIO', '_', 'Toggle', 'Pin', 'G', 'PIO', '_', 'Port', 'Ġ13']\nMatching Tokens: {'G', '_', 'H', 'PIO', 'AL', 'Toggle', 'Pin'}\nAccuracy: 0.5385\n\nExample 9:\nPrompt: Generate STM32 code to initialize the network stack.\nTarget: net_init();\nPredicted: HAL_Network_Init();\nTarget Tokens: ['net', '_', 'init']\nPredicted Tokens: ['H', 'AL', '_', 'Network', '_', 'Init']\nMatching Tokens: {'_'}\nAccuracy: 0.3333\n\nExample 10:\nPrompt: Generate STM32 code to start the ADC conversion.\nTarget: HAL_ADC_Start(&hadc1);\nPredicted: HAL_ADC_Init();\nTarget Tokens: ['H', 'AL', '_', 'AD', 'C', '_', 'Start', 'h', 'ad', 'c', '1']\nPredicted Tokens: ['H', 'AL', '_', 'AD', 'C', '_', 'Init']\nMatching Tokens: {'_', 'H', 'C', 'AL', 'AD'}\nAccuracy: 0.5000\n\nOverall Token Accuracy: 0.4523\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from transformers import RobertaTokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\nimport nltk\nfrom nltk.metrics.distance import edit_distance\nimport torch\nfrom nltk.tokenize import word_tokenize\n\n# Download tokenizer data\nnltk.download('punkt')\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load tokenizer and model (make sure these are defined before this block)\n# tokenizer = ...\n# model = ...\nmodel = model.to(device)\n\n# Original prompt\nprompt = \"[Instruction] Generate STM32 code for the following description: initialize GPIO for STM32\"\noriginal_inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=32, truncation=True)\noriginal_inputs = {k: v.to(device) for k, v in original_inputs.items()}  # MOVE BEFORE GENERATE\n\nwith torch.no_grad():\n    original_outputs = model.generate(**original_inputs, max_length=100, num_beams=3, no_repeat_ngram_size=2, early_stopping=True)\noriginal_code = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\nprint(\"Original code:\", original_code)\n\n# Tokenize prompt for perturbation\nprompt_words = word_tokenize(prompt)\nprint(\"Prompt words:\", prompt_words)\n\n# Perturb by removing one word at a time\nperturbed_codes = []\nfor i in range(len(prompt_words)):\n    perturbed_words = prompt_words[:i] + prompt_words[i+1:]\n    perturbed_prompt = \" \".join(perturbed_words)\n    inputs = tokenizer(perturbed_prompt, return_tensors=\"pt\", max_length=32, truncation=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=100, num_beams=3, no_repeat_ngram_size=2, early_stopping=True)\n    perturbed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    edit_dist = edit_distance(original_code, perturbed_code)\n    perturbed_codes.append((perturbed_prompt, perturbed_code, edit_dist))\n\n# Print results\nprint(\"\\nPerturbation Analysis:\")\nfor perturbed_prompt, perturbed_code, edit_dist in perturbed_codes:\n    print(f\"Perturbed Prompt: {perturbed_prompt}\")\n    print(f\"Perturbed Code: {perturbed_code}\")\n    print(f\"Edit Distance: {edit_dist}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom captum.attr import IntegratedGradients\nimport torch\n\n# Load model and tokenizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Input prompt\nprompt = \"[Instruction] Generate STM32 code for the following description: initialize GPIO for STM32\"\ninputs = tokenizer(prompt, return_tensors=\"pt\", max_length=32, truncation=True)\n\n# Debugging: Check tensor type right after tokenization\nprint(f\"input_ids dtype before transfer: {inputs['input_ids'].dtype}\")\n\n# Move inputs to device and cast to long (if needed)\ninput_ids = inputs[\"input_ids\"].to(device).long()  # Ensure it's long dtype\nattention_mask = inputs[\"attention_mask\"].to(device).long()  # Same for attention_mask\n\n# Debugging: Check tensor type after transfer and cast\nprint(f\"input_ids dtype after transfer: {input_ids.dtype}\")\nprint(f\"attention_mask dtype after transfer: {attention_mask.dtype}\")\n\n# Forward function for Integrated Gradients\ndef forward_func(input_ids, attention_mask):\n    # Debugging: Check tensor types inside forward pass\n    print(f\"Inside forward_func - input_ids dtype: {input_ids.dtype}, attention_mask dtype: {attention_mask.dtype}\")\n    \n    # Ensure correct tensor types in forward pass\n    assert input_ids.dtype == torch.long, f\"Expected input_ids to be of type torch.long, but got {input_ids.dtype}\"\n    assert attention_mask.dtype == torch.long, f\"Expected attention_mask to be of type torch.long, but got {attention_mask.dtype}\"\n    \n    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n    return outputs.logits\n\n# Integrated Gradients attribution\nig = IntegratedGradients(forward_func)\n\n# Compute attributions\nattributions, delta = ig.attribute(\n    inputs=input_ids,\n    additional_forward_args=(attention_mask,),\n    target=0,  # Attribution for first token\n    return_convergence_delta=True\n)\n\n# Sum attributions across embedding dimensions\nattributions = attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\ninput_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n\n# Print token importance\nprint(\"Token Importance:\")\nfor token, score in zip(input_tokens, attributions):\n    print(f\"Token: {token}, Score: {score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install shap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:10:03.377479Z","iopub.execute_input":"2025-05-11T19:10:03.377993Z","iopub.status.idle":"2025-05-11T19:10:06.388865Z","shell.execute_reply.started":"2025-05-11T19:10:03.377962Z","shell.execute_reply":"2025-05-11T19:10:06.387899Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.5.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.3)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->shap) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->shap) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->shap) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->shap) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->shap) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->shap) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"pip install sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:10:06.390288Z","iopub.execute_input":"2025-05-11T19:10:06.390525Z","iopub.status.idle":"2025-05-11T19:10:07.976746Z","shell.execute_reply.started":"2025-05-11T19:10:06.390504Z","shell.execute_reply":"2025-05-11T19:10:07.975930Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement sentence_bleu (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for sentence_bleu\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import shap\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:02:20.670619Z","iopub.execute_input":"2025-05-11T21:02:20.670859Z","iopub.status.idle":"2025-05-11T21:02:20.674611Z","shell.execute_reply.started":"2025-05-11T21:02:20.670842Z","shell.execute_reply":"2025-05-11T21:02:20.673738Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.config.output_attentions=True\nsample = test_dataset[0]\n\n# 1. Attention Visualization\ndef visualize_attention(input_text, target_text, model, tokenizer, layer_idx=0, head_idx=0):\n    # Ensure model is configured to output attentions\n    model.config.output_attentions = True\n    \n    # Tokenize input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # Use model.generate to get attentions\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        output_attentions=True,\n        return_dict_in_generate=True,\n        max_length=64\n    )\n    \n    # Debug: Print output keys and attentions structure\n    print(\"Model output keys:\", outputs.keys())\n    if \"cross_attentions\" in outputs and outputs.cross_attentions is not None:\n        cross_attention_tuple = outputs.cross_attentions[layer_idx]\n        print(\"Cross attention tuple type:\", type(cross_attention_tuple))\n        if isinstance(cross_attention_tuple, tuple):\n            print(\"Cross attention tuple length:\", len(cross_attention_tuple))\n            print(\"First element type:\", type(cross_attention_tuple[0]))\n            if hasattr(cross_attention_tuple[0], \"shape\"):\n                print(\"First element shape:\", cross_attention_tuple[0].shape)\n            cross_attention = cross_attention_tuple[0][0][head_idx]\n            print(\"Extracted cross_attention shape:\", cross_attention.shape)\n        else:\n            print(\"Cross attention tensor shape:\", cross_attention_tuple.shape)\n            ross_attention = cross_attention_tuple[0][head_idx]\n            print(\"Extracted cross_attention shape:\", cross_attention.shape)\n    \n    # Get input and output tokens\n    input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    output_tokens = tokenizer.convert_ids_to_tokens(outputs.sequences[0])\n    \n    # Plot heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(cross_attention.cpu().detach().numpy(), xticklabels=input_tokens, yticklabels=output_tokens, cmap=\"viridis\")\n    plt.title(f\"Cross-Attention (Layer {layer_idx}, Head {head_idx})\")\n    plt.xlabel(\"Input Tokens\")\n    plt.ylabel(\"Output Tokens\")\n    plt.savefig(\"attention_heatmap.png\")\n    plt.close()\n    \n    # Metric: Attention Entropy\n    attention_probs = cross_attention.cpu().detach().numpy()\n    entropy = -np.sum(attention_probs * np.log(attention_probs + 1e-10), axis=1).mean()\n    return {\"attention_entropy\": entropy}\n\n\n\n#visualize_attention(input_text, target_text, model, tokenizer, layer_idx=0, head_idx=0)\n\n# 2. Integrated Gradients\ndef integrated_gradients_analysis(input_text, model, tokenizer):\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    input_ids = inputs[\"input_ids\"]\n    embeddings = model.get_input_embeddings()(input_ids)\n    \n    print(\"Input IDs shape:\", input_ids.shape)\n    print(\"Embeddings shape:\", embeddings.shape)\n    print(\"Labels shape:\", inputs[\"input_ids\"].shape)\n    \n    def forward_func(embeddings):\n        # Provide decoder_input_ids explicitly\n        outputs = model(\n            inputs_embeds=embeddings,\n            decoder_input_ids=inputs[\"input_ids\"],  # Use input_ids as decoder input\n            output_attentions=True\n        )\n        return outputs.logits[:, -1, :].softmax(dim=-1)\n    \n    from captum.attr import IntegratedGradients\n    ig = IntegratedGradients(forward_func)\n    baseline = torch.zeros_like(embeddings)\n    attributions, delta = ig.attribute(embeddings, baseline, target=0, return_convergence_delta=True)\n    \n    # Debug: Print shapes\n    print(\"Attributions shape:\", attributions.shape)\n    print(\"After sum(dim=-1):\", attributions.sum(dim=-1).shape)\n    print(\"After squeeze():\", attributions.sum(dim=-1).squeeze().shape)\n    \n    # Sum attributions across embedding dimensions and ensure 1D output\n    attribution_scores = attributions.sum(dim=-1).squeeze().cpu().detach().numpy()\n    if attribution_scores.ndim > 1:\n        attribution_scores = attribution_scores.flatten()\n    \n    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    completeness = abs(delta.cpu().detach().numpy().sum())\n    \n    return {\"tokens\": input_tokens, \"attribution_scores\": attribution_scores, \"completeness\": completeness}\n\n# 3. SHAP Analysis\ndef shap_analysis(input_text, model, tokenizer):\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    def model_predict(texts):\n        tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        with torch.no_grad():\n            outputs = model(**tokenized,output_attentions=True)\n        return outputs.logits.softmax(dim=-1).cpu().numpy()\n    \n    explainer = shap.KernelExplainer(model_predict, [tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)])\n    shap_values = explainer.shap_values(input_text)\n    \n    # Metric: Mean Absolute SHAP\n    mean_abs_shap = np.abs(shap_values).mean()\n    \n    return {\"shap_values\": shap_values, \"mean_abs_shap\": mean_abs_shap}\n\n# 4. Perturbation Analysis\ndef perturbation_analysis(input_text, target_text, model, tokenizer):\n    input_tokens = input_text.split()\n    results = []\n    \n    for i, token in enumerate(input_tokens):\n        # Remove one token\n        perturbed_input = \" \".join(input_tokens[:i] + input_tokens[i+1:])\n        perturbed_inputs = tokenizer(perturbed_input, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        with torch.no_grad():\n            perturbed_output = model.generate(**perturbed_inputs, max_length=64)\n        perturbed_code = tokenizer.decode(perturbed_output[0], skip_special_tokens=True)\n        \n        # BLEU Score\n        reference = target_text.split()\n        candidate = perturbed_code.split()\n        bleu = sentence_bleu([reference], candidate)\n        \n        # Domain-Specific Check (Placeholder: Validate STM32 syntax)\n        is_valid = True  # Replace with actual C compiler check for STM32\n        results.append({\"token_removed\": token, \"bleu_score\": bleu, \"valid_code\": is_valid})\n    \n    return results\n\n# 5. Qualitative Case Study\ndef qualitative_case_study(test_dataset, model, tokenizer, num_examples=5):\n    examples = test_dataset.select(range(min(num_examples, len(test_dataset))))\n    analysis_results = []\n    \n    for example in examples:\n        input_text = example[\"input_text\"]\n        target_text = example[\"target_text\"]\n        \n        # Run all analyses\n        attention_metrics = visualize_attention(input_text, target_text, model, tokenizer)\n        #ig_metrics = integrated_gradients_analysis(input_text, model, tokenizer)\n        #shap_metrics = shap_analysis(input_text, model, tokenizer)\n        perturbation_metrics = perturbation_analysis(input_text, target_text, model, tokenizer)\n        \n        result = {\n            \"input_text\": example[\"input_text\"],\n            \"target_text\": example[\"target_text\"],\n            \"attention_entropy\": attention_metrics[\"attention_entropy\"],\n            #\"ig_attributions\": ig_metrics[\"attribution_scores\"],\n            #\"ig_completeness\": ig_metrics[\"completeness\"],\n            #\"shap_mean_abs\": shap_metrics[\"mean_abs_shap\"],\n            \"perturbation_results\": perturbation_metrics\n        }\n        \n        # Debug: Print types and shapes\n        print(\"Result dictionary:\")\n        for key, value in result.items():\n            print(f\"{key}: type={type(value)}\", end=\"\")\n            if isinstance(value, np.ndarray):\n                print(f\", shape={value.shape}\")\n            elif isinstance(value, list):\n                print(f\", length={len(value)}\")\n                if value and isinstance(value[0], dict):\n                    print(f\"  First item keys: {list(value[0].keys())}\")\n            else:\n                print()\n        \n        analysis_results.append(result)\n    \n    # Convert to DataFrame and save\n    import pandas as pd\n    df = pd.DataFrame(analysis_results)\n    print(\"DataFrame columns:\", df.columns)\n    print(\"DataFrame head:\\n\", df.head())\n    df.to_csv(\"xai_analysis_results.csv\", index=False)\n    return analysis_results\n\n# Run XAI Analysis\nxai_results = qualitative_case_study(test_dataset, model, tokenizer, num_examples=5)\nprint(\"XAI Analysis Completed. Results saved to 'xai_analysis_results.csv'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:02:26.330993Z","iopub.execute_input":"2025-05-11T21:02:26.331808Z","iopub.status.idle":"2025-05-11T21:02:43.608303Z","shell.execute_reply.started":"2025-05-11T21:02:26.331779Z","shell.execute_reply":"2025-05-11T21:02:43.607663Z"}},"outputs":[{"name":"stdout","text":"Model output keys: odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\nCross attention tuple type: <class 'tuple'>\nCross attention tuple length: 12\nFirst element type: <class 'torch.Tensor'>\nFirst element shape: torch.Size([1, 12, 1, 14])\nExtracted cross_attention shape: torch.Size([1, 14])\nResult dictionary:\ninput_text: type=<class 'str'>\ntarget_text: type=<class 'str'>\nattention_entropy: type=<class 'numpy.float32'>\nperturbation_results: type=<class 'list'>, length=9\n  First item keys: ['token_removed', 'bleu_score', 'valid_code']\nModel output keys: odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\nCross attention tuple type: <class 'tuple'>\nCross attention tuple length: 12\nFirst element type: <class 'torch.Tensor'>\nFirst element shape: torch.Size([1, 12, 1, 14])\nExtracted cross_attention shape: torch.Size([1, 14])\nResult dictionary:\ninput_text: type=<class 'str'>\ntarget_text: type=<class 'str'>\nattention_entropy: type=<class 'numpy.float32'>\nperturbation_results: type=<class 'list'>, length=9\n  First item keys: ['token_removed', 'bleu_score', 'valid_code']\nModel output keys: odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\nCross attention tuple type: <class 'tuple'>\nCross attention tuple length: 12\nFirst element type: <class 'torch.Tensor'>\nFirst element shape: torch.Size([1, 12, 1, 14])\nExtracted cross_attention shape: torch.Size([1, 14])\nResult dictionary:\ninput_text: type=<class 'str'>\ntarget_text: type=<class 'str'>\nattention_entropy: type=<class 'numpy.float32'>\nperturbation_results: type=<class 'list'>, length=9\n  First item keys: ['token_removed', 'bleu_score', 'valid_code']\nModel output keys: odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\nCross attention tuple type: <class 'tuple'>\nCross attention tuple length: 12\nFirst element type: <class 'torch.Tensor'>\nFirst element shape: torch.Size([1, 12, 1, 14])\nExtracted cross_attention shape: torch.Size([1, 14])\nResult dictionary:\ninput_text: type=<class 'str'>\ntarget_text: type=<class 'str'>\nattention_entropy: type=<class 'numpy.float32'>\nperturbation_results: type=<class 'list'>, length=9\n  First item keys: ['token_removed', 'bleu_score', 'valid_code']\nModel output keys: odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])\nCross attention tuple type: <class 'tuple'>\nCross attention tuple length: 12\nFirst element type: <class 'torch.Tensor'>\nFirst element shape: torch.Size([1, 12, 1, 14])\nExtracted cross_attention shape: torch.Size([1, 14])\nResult dictionary:\ninput_text: type=<class 'str'>\ntarget_text: type=<class 'str'>\nattention_entropy: type=<class 'numpy.float32'>\nperturbation_results: type=<class 'list'>, length=9\n  First item keys: ['token_removed', 'bleu_score', 'valid_code']\nDataFrame columns: Index(['input_text', 'target_text', 'attention_entropy',\n       'perturbation_results'],\n      dtype='object')\nDataFrame head:\n                                           input_text  \\\n0  Generate STM32 code for the following descript...   \n1  Generate STM32 code for the following descript...   \n2  Generate STM32 code for the following descript...   \n3  Generate STM32 code for the following descript...   \n4  Generate STM32 code for the following descript...   \n\n                                         target_text  attention_entropy  \\\n0                                   osKernelStart();           0.938192   \n1  HAL_UART_Transmit(&huart1, serial_data, 3, HAL...           0.966361   \n2                                        net_init();           0.950890   \n3                           ketCube_terminal_Init();           0.953451   \n4                           arm_pid_init_f32(&pid,1)           0.951919   \n\n                                perturbation_results  \n0  [{'token_removed': 'Generate', 'bleu_score': 0...  \n1  [{'token_removed': 'Generate', 'bleu_score': 0...  \n2  [{'token_removed': 'Generate', 'bleu_score': 0...  \n3  [{'token_removed': 'Generate', 'bleu_score': 0...  \n4  [{'token_removed': 'Generate', 'bleu_score': 0...  \nXAI Analysis Completed. Results saved to 'xai_analysis_results.csv'.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!zip xai_analysis_results.zip xai_analysis_results.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T21:03:13.486367Z","iopub.execute_input":"2025-05-11T21:03:13.486644Z","iopub.status.idle":"2025-05-11T21:03:13.663469Z","shell.execute_reply.started":"2025-05-11T21:03:13.486623Z","shell.execute_reply":"2025-05-11T21:03:13.662535Z"}},"outputs":[{"name":"stdout","text":"  adding: xai_analysis_results.csv (deflated 88%)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(\"Output attentions enabled:\", model.config.output_attentions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Metric: Attention Entropy\n    attention_probs = cross_attention.cpu().detach().numpy()\n    entropy = -np.sum(attention_probs * np.log(attention_probs + 1e-10), axis=1).mean()\n    return {\"attention_entropy\": entropy}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install lime","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJacAmWeZL6_","outputId":"eb3abfbe-a243-4ca8-ddd8-9f26837e0e95","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lime\nfrom lime.lime_text import LimeTextExplainer\nimport torch\nimport numpy as np\n\n# Define a prediction function for LIME\ndef predict_proba(texts):\n    # Tokenize input texts\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Generate code (get logits)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits.cpu().numpy()\n\n    # Convert logits to probabilities (simplified for LIME)\n    probs = np.softmax(logits, axis=-1)\n    return probs\n\n# Initialize LIME explainer\nexplainer = LimeTextExplainer(class_names=[\"code_output\"])  # Simplified for single output\n\n# Select a test comment\ntest_comment = test_dataset[0][\"input_text\"]  # e.g., \"[Instruction] Generate STM32 code for GPIO toggle\"\n\n# Generate LIME explanation\nexplanation = explainer.explain_instance(test_comment, predict_proba, num_features=10)\n\n# Display explanation\nexplanation.show_in_notebook()  # Or print(explanation.as_list())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"sj1sPLN5ZH1J","outputId":"437f7745-ea92-46b3-a308-49974569ebd4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# Select a test comment and tokenize\ntest_comment = test_dataset[0][\"input_text\"]\ninputs = tokenizer(test_comment, return_tensors=\"pt\").to(model.device)\n\n# Get attention weights\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**inputs, output_attentions=True)\n    attentions = outputs.attentions  # List of attention matrices per layer\n\n# Average attention weights across heads and layers (simplified)\navg_attention = torch.stack(attentions).mean(dim=0).mean(dim=1)[0].cpu().numpy()\n\n# Get token labels\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Plot attention heatmap\nplt.figure(figsize=(10, 8))\nplt.imshow(avg_attention, cmap=\"viridis\")\nplt.xticks(range(len(tokens)), tokens, rotation=45)\nplt.yticks(range(len(tokens)), tokens)\nplt.colorbar()\nplt.title(\"Attention Heatmap for Comment\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"BgIiOO9Iamoo","outputId":"472364c8-7f85-4d98-eed0-dbaeb68f4f88","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"id":"QYGyYFe7a5Jg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your model and tokenizer\ngenerator = pipeline(\"text-generation\", model=\"your_model_name_here\")\n\n# Define test prompts\nprompts = [\n    # Basic Functional Prompts\n    \"Comment: Blink an LED connected to pin PA5 at 1 Hz\",\n    \"Comment: Configure SPI1 in master mode\",\n    \"Comment: Send the string 'Hello' over USART2\",\n    \"Comment: Set GPIO pin PB3 as output\",\n\n    # Detailed/Explanatory Prompts\n    \"Comment: Implement a function that waits for 2 seconds using HAL delay\",\n    \"Comment: Initialize the ADC1 peripheral and start a conversion\",\n    \"Comment: Create a function to toggle LED2 every 250ms\",\n    \"Comment: Configure the system clock using HSE with PLL\",\n\n    # Action-Oriented Prompts\n    \"Comment: Toggle the onboard LED\",\n    \"Comment: Start PWM on TIM2 Channel 1\",\n    \"Comment: Reset the watchdog timer\",\n    \"Comment: Enable interrupts for EXTI line 13\",\n\n    # Structured/Formal Prompts\n    \"Comment: void Init_I2C1(void) — initialize I2C1 with 100kHz standard mode\",\n    \"Comment: void Transmit_UART1(char* data) — send a null-terminated string over UART1\",\n    \"Comment: void GPIO_SetPin(GPIO_TypeDef* GPIOx, uint16_t GPIO_Pin) — set a GPIO pin high\",\n\n    # Error/Edge Case Prompts\n    \"Comment: Try to write to a NULL pointer\",\n    \"Comment: Handle timeout if I2C transmission fails\",\n    \"Comment: Return an error if UART buffer is full\",\n]\n\n# Function to test each prompt\ndef test_prompts(prompts):\n    for prompt in prompts:\n        print(f\"Testing prompt: {prompt}\\n\")\n        response = generator(prompt, max_length=200, num_return_sequences=1)\n        print(f\"Generated Code: {response[0]['generated_text']}\\n\")\n        print(\"=\"*80)\n\n# Run tests\ntest_prompts(prompts)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"id":"6UChOM8ga0qY","outputId":"0248de16-e11e-4a62-b26d-f76c4203de43","trusted":true},"outputs":[],"execution_count":null}]}